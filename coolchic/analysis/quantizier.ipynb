{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b81455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: 2025_11_22__16_44_29__trained_coolchic_kodak_kodim01_img_rate_3.2631609439849854.pth\n",
      "Converting image to YCoCg color space\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CoolChicEncoder:\n\tsize mismatch for synthesis.layers.2.weight: copying a param with shape torch.Size([9, 24, 1, 1]) from checkpoint, the shape in current model is torch.Size([6, 24, 1, 1]).\n\tsize mismatch for synthesis.layers.2.bias: copying a param with shape torch.Size([9]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for synthesis.layers.4.weight: copying a param with shape torch.Size([9, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([6, 2, 3, 3]).\n\tsize mismatch for synthesis.layers.4.bias: copying a param with shape torch.Size([9]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for synthesis.layers.6.weight: copying a param with shape torch.Size([9, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([6, 2, 3, 3]).\n\tsize mismatch for synthesis.layers.6.bias: copying a param with shape torch.Size([9]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for image_arm.models.0.0.weight: copying a param with shape torch.Size([6, 33]) from checkpoint, the shape in current model is torch.Size([6, 30]).\n\tsize mismatch for image_arm.models.1.0.weight: copying a param with shape torch.Size([6, 34]) from checkpoint, the shape in current model is torch.Size([6, 31]).\n\tsize mismatch for image_arm.models.1.4.weight: copying a param with shape torch.Size([6, 6]) from checkpoint, the shape in current model is torch.Size([4, 6]).\n\tsize mismatch for image_arm.models.1.4.bias: copying a param with shape torch.Size([6]) from checkpoint, the shape in current model is torch.Size([4]).\n\tsize mismatch for image_arm.models.2.0.weight: copying a param with shape torch.Size([6, 35]) from checkpoint, the shape in current model is torch.Size([6, 32]).\n\tsize mismatch for image_arm.models.2.4.weight: copying a param with shape torch.Size([8, 6]) from checkpoint, the shape in current model is torch.Size([4, 6]).\n\tsize mismatch for image_arm.models.2.4.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([4]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m coolchic \u001b[38;5;241m=\u001b[39m CoolChicEncoder(param\u001b[38;5;241m=\u001b[39mencoder_param)\n\u001b[1;32m     50\u001b[0m coolchic\u001b[38;5;241m.\u001b[39mto_device(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mcoolchic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_location_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# ==========================================================================================\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# QUANTIZE AND EVALUATE\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# ==========================================================================================\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# technically we don't need quantization when working with uncompressed model\u001b[39;00m\n\u001b[1;32m     57\u001b[0m quantized_coolchic \u001b[38;5;241m=\u001b[39m CoolChicEncoder(param\u001b[38;5;241m=\u001b[39mencoder_param)\n",
      "File \u001b[0;32m~/miniconda3/envs/cool_chic_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2624\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2616\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2617\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2618\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2619\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2620\u001b[0m             ),\n\u001b[1;32m   2621\u001b[0m         )\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2626\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2627\u001b[0m         )\n\u001b[1;32m   2628\u001b[0m     )\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CoolChicEncoder:\n\tsize mismatch for synthesis.layers.2.weight: copying a param with shape torch.Size([9, 24, 1, 1]) from checkpoint, the shape in current model is torch.Size([6, 24, 1, 1]).\n\tsize mismatch for synthesis.layers.2.bias: copying a param with shape torch.Size([9]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for synthesis.layers.4.weight: copying a param with shape torch.Size([9, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([6, 2, 3, 3]).\n\tsize mismatch for synthesis.layers.4.bias: copying a param with shape torch.Size([9]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for synthesis.layers.6.weight: copying a param with shape torch.Size([9, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([6, 2, 3, 3]).\n\tsize mismatch for synthesis.layers.6.bias: copying a param with shape torch.Size([9]) from checkpoint, the shape in current model is torch.Size([6]).\n\tsize mismatch for image_arm.models.0.0.weight: copying a param with shape torch.Size([6, 33]) from checkpoint, the shape in current model is torch.Size([6, 30]).\n\tsize mismatch for image_arm.models.1.0.weight: copying a param with shape torch.Size([6, 34]) from checkpoint, the shape in current model is torch.Size([6, 31]).\n\tsize mismatch for image_arm.models.1.4.weight: copying a param with shape torch.Size([6, 6]) from checkpoint, the shape in current model is torch.Size([4, 6]).\n\tsize mismatch for image_arm.models.1.4.bias: copying a param with shape torch.Size([6]) from checkpoint, the shape in current model is torch.Size([4]).\n\tsize mismatch for image_arm.models.2.0.weight: copying a param with shape torch.Size([6, 35]) from checkpoint, the shape in current model is torch.Size([6, 32]).\n\tsize mismatch for image_arm.models.2.4.weight: copying a param with shape torch.Size([8, 6]) from checkpoint, the shape in current model is torch.Size([4, 6]).\n\tsize mismatch for image_arm.models.2.4.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([4])."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"analysis\":\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from lossless.component.coolchic import CoolChicEncoder, CoolChicEncoderParameter\n",
    "from lossless.nnquant.quantizemodel import quantize_model\n",
    "from lossless.training.loss import loss_function\n",
    "from lossless.training.manager import ImageEncoderManager\n",
    "from lossless.util.config import args\n",
    "from lossless.util.image_loading import load_image_as_tensor\n",
    "from lossless.util.parsecli import (\n",
    "    change_n_out_synth,\n",
    "    get_coolchic_param_from_args,\n",
    "    get_manager_from_args,\n",
    ")\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model_location_dir = \"../logs_cluster/logs/full_runs/21_11_2025_YCoCg_with_fixed_colorregression/trained_models\"\n",
    "model_paths = os.listdir(model_location_dir)\n",
    "model_paths = sorted(model_paths, key=lambda x: int(x.split(\"_kodim\")[1].split(\"_\")[0]))\n",
    "\n",
    "table = []\n",
    "for img_index in range(len(model_paths)):\n",
    "    color_space = \"YCoCg\"\n",
    "    use_image_arm = True\n",
    "    print(f\"Using model: {model_paths[img_index]}\")\n",
    "\n",
    "    im_path = args[\"input\"][img_index]\n",
    "    im_tensor, c_bitdepths = load_image_as_tensor(im_path, device=\"cuda:0\", color_space=color_space)\n",
    "\n",
    "    # ==========================================================================================\n",
    "    # LOAD PRESETS, COOLCHIC PARAMETERS\n",
    "    # ==========================================================================================\n",
    "    image_encoder_manager = ImageEncoderManager(**get_manager_from_args(args))\n",
    "    encoder_param = CoolChicEncoderParameter(**get_coolchic_param_from_args(args, \"lossless\"))\n",
    "    encoder_param.set_image_size((im_tensor.shape[2], im_tensor.shape[3]))\n",
    "    encoder_param.layers_synthesis = change_n_out_synth(\n",
    "        encoder_param.layers_synthesis, args[\"output_dim_size\"]\n",
    "    )\n",
    "    encoder_param.use_image_arm = use_image_arm\n",
    "    coolchic = CoolChicEncoder(param=encoder_param)\n",
    "    coolchic.to_device(\"cuda:0\")\n",
    "    coolchic.load_state_dict(torch.load(os.path.join(model_location_dir, model_paths[img_index])))\n",
    "\n",
    "    # ==========================================================================================\n",
    "    # QUANTIZE AND EVALUATE\n",
    "    # ==========================================================================================\n",
    "    # technically we don't need quantization when working with uncompressed model\n",
    "    quantized_coolchic = CoolChicEncoder(param=encoder_param)\n",
    "    quantized_coolchic.to_device(\"cuda:0\")\n",
    "    quantized_coolchic.set_param(coolchic.get_param())\n",
    "    # quantized_coolchic = quantize_model(\n",
    "    #     quantized_coolchic,\n",
    "    #     im_tensor,\n",
    "    #     image_encoder_manager,\n",
    "    #     None, # type:ignore\n",
    "    #     color_bitdepths=c_bitdepths,\n",
    "    # )\n",
    "    # rate_per_module, total_network_rate = quantized_coolchic.get_network_rate()\n",
    "    with torch.no_grad():\n",
    "        arm_params = list(quantized_coolchic.image_arm.parameters())\n",
    "        arm_params_bits = sum(p.numel() for p in arm_params) * 32  # assuming float32\n",
    "    print(arm_params_bits / im_tensor.numel())\n",
    "    raise Exception(\"Stop for testing.\")\n",
    "    total_network_rate += arm_params_bits\n",
    "    total_network_rate /= im_tensor.numel()\n",
    "    total_network_rate = float(total_network_rate)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass with no quantization noise\n",
    "        predicted_prior = quantized_coolchic.forward(\n",
    "            image=im_tensor,\n",
    "            quantizer_noise_type=\"none\",\n",
    "            quantizer_type=\"hardround\",\n",
    "            AC_MAX_VAL=-1,\n",
    "            flag_additional_outputs=False,\n",
    "        )\n",
    "        predicted_priors_rates = loss_function(\n",
    "            predicted_prior,\n",
    "            im_tensor,\n",
    "            rate_mlp_bpd=total_network_rate,\n",
    "            latent_multiplier=1.0,\n",
    "            channel_ranges=c_bitdepths,\n",
    "        )\n",
    "    print(\n",
    "        f\"Rate per module: {rate_per_module},\\n\",\n",
    "        f\"Final results after quantization: {predicted_priors_rates}\"\n",
    "    )\n",
    "    table.append(\n",
    "        {\n",
    "            \"Index\": img_index,\n",
    "            \"Loss\": predicted_priors_rates.loss.cpu().item(),\n",
    "            \"Rate NN\": predicted_priors_rates.rate_nn_bpd,\n",
    "            \"Rate Latent\": predicted_priors_rates.rate_latent_bpd,\n",
    "            \"Rate Img\": predicted_priors_rates.rate_img_bpd,\n",
    "        }\n",
    "    )\n",
    "    raise Exception(\"Stop after one iteration for testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3969ed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LaTeX Table:\n",
      "\n",
      "\\begin{tabular}{lllll}\n",
      "Index & Loss & Rate NN & Rate Latent & Rate Img \\\\\n",
      "\\hline\n",
      "0 & 4.178 & 0.029 & 0.371 & 3.778 \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "def dict_list_to_latex_table(rows, floatfmt=\"{:.3f}\"):\n",
    "    \"\"\"\n",
    "    Convert a list of dicts into a LaTeX table string.\n",
    "    \n",
    "    Args:\n",
    "        rows (list of dict): All dicts must have the same keys.\n",
    "        floatfmt (str): Format string for floats, e.g. \"{:.4f}\".\n",
    "    \n",
    "    Returns:\n",
    "        str: LaTeX table.\n",
    "    \"\"\"\n",
    "    if not rows:\n",
    "        return \"\"\n",
    "\n",
    "    # Column names taken from dict keys\n",
    "    cols = list(rows[0].keys())\n",
    "\n",
    "    # Escape LaTeX special characters in column names\n",
    "    def escape(s):\n",
    "        repl = {\n",
    "            '%': r'\\%',\n",
    "            '&': r'\\&',\n",
    "            '_': r'\\_',\n",
    "        }\n",
    "        for k, v in repl.items():\n",
    "            s = s.replace(k, v)\n",
    "        return s\n",
    "\n",
    "    header = \" & \".join(escape(c) for c in cols) + r\" \\\\\"\n",
    "\n",
    "    # Build table rows\n",
    "    body_lines = []\n",
    "    for row in rows:\n",
    "        cells = []\n",
    "        for c in cols:\n",
    "            v = row[c]\n",
    "            if isinstance(v, float):\n",
    "                v = floatfmt.format(v)\n",
    "            cells.append(str(v))\n",
    "        body_lines.append(\" & \".join(cells) + r\" \\\\\")\n",
    "    \n",
    "    body = \"\\n\".join(body_lines)\n",
    "\n",
    "    # Combine into a LaTeX table\n",
    "    latex = (\n",
    "        \"\\\\begin{tabular}{%s}\\n\" % (\"l\" * len(cols)) +\n",
    "        header + \"\\n\\\\hline\\n\" +\n",
    "        body + \"\\n\\\\end{tabular}\"\n",
    "    )\n",
    "    return latex\n",
    "\n",
    "latex_table = dict_list_to_latex_table(table, floatfmt=\"{:.3f}\")\n",
    "print(\"\\nLaTeX Table:\\n\")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be54199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Averages:\n",
      "Average Loss: 4.178029\n",
      "Average Rate NN: 0.028973\n",
      "Average Rate Latent: 0.370850\n",
      "Average Rate Img: 3.778206\n"
     ]
    }
   ],
   "source": [
    "average_loss = np.mean([row[\"Loss\"] for row in table])\n",
    "average_rate_nn = np.mean([row[\"Rate NN\"] for row in table])\n",
    "average_rate_latent = np.mean([row[\"Rate Latent\"] for row in table])\n",
    "average_rate_img = np.mean([row[\"Rate Img\"] for row in table])\n",
    "print(\"\\nAverages:\")\n",
    "print(f\"Average Loss: {average_loss:.6f}\")\n",
    "print(f\"Average Rate NN: {average_rate_nn:.6f}\")\n",
    "print(f\"Average Rate Latent: {average_rate_latent:.6f}\")\n",
    "print(f\"Average Rate Img: {average_rate_img:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cool_chic_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
