{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f6287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b437a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"testing\":\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "import unittest\n",
    "from lossless.util.distribution import (\n",
    "    weak_colorar_rate,\n",
    "    discretized_logistic_logp,\n",
    "    get_mu_and_scale_linear_color\n",
    ")\n",
    "import lossless.util.color_transform as color_transform\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams[\"figure.dpi\"] = 150\n",
    "\n",
    "def unget_scale(scale: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Helper function to get log_scale from scale\n",
    "\n",
    "    Args:\n",
    "        - scale (torch.Tensor): Tensor of shape N x C x H x W\n",
    "    \"\"\"\n",
    "    logscale = torch.log(scale) * -2\n",
    "    logscale = torch.clamp(logscale, min=-10.0, max=13.8155)\n",
    "    return logscale\n",
    "\n",
    "_ = (\n",
    "# def inflate_mu_and_scale_linear_color(\n",
    "#     mu: torch.Tensor, log_scale: torch.Tensor, x: torch.Tensor\n",
    "# ) -> torch.Tensor:\n",
    "#     \"\"\"Helper function to get params from mu and scale\n",
    "\n",
    "#     Args:\n",
    "#         - mu (torch.Tensor): Tensor of shape N x 3 x H x W\n",
    "#         - log_scale (torch.Tensor): Tensor of shape N x 3 x H x W\n",
    "#         - x (torch.Tensor): Tensor of shape N x 3 x H x W\n",
    "#     \"\"\"\n",
    "#     pass\n",
    "\n",
    "\n",
    "# class TestWeakColorarRate(unittest.TestCase):\n",
    "#     def simple_case(self):\n",
    "#         channel_ranges = color_transform.RGBBitdepths()\n",
    "#         x = torch.tensor([[[[0.40802]], [[0.24332]], [[0.63202]]]])\n",
    "#         print(x.shape)\n",
    "#         mu = x + 0.1\n",
    "#         logscale = torch.tensor([[[[0.0]], [[0.0]], [[0.0]]]])\n",
    "#         params = inflate_mu_and_scale_linear_color(mu, logscale, x)\n",
    "\n",
    "# TestWeakColorarRate().simple_case()\n",
    ")\n",
    "\n",
    "class MonochromeBitdepths(color_transform.ColorBitdepths):\n",
    "    def __init__(self) -> None:\n",
    "        self.bitdepths = [8]\n",
    "        self.scaling_factors = [255]\n",
    "        self.bins = [256]\n",
    "        self.ranges_int = [[0, 255]]\n",
    "\n",
    "\n",
    "DEFAULT_LOGSCALE = 13.4\n",
    "DEFAULT_SHIFT = 0.0\n",
    "DEFAULT_IMG_VALUE = 0.00802\n",
    "\n",
    "\n",
    "def plot_logp(shifts: np.ndarray | None, logscales: np.ndarray | None):\n",
    "    channel_ranges = MonochromeBitdepths()\n",
    "    x = torch.ones((1, 1, 1, 1)) * DEFAULT_IMG_VALUE\n",
    "\n",
    "    # shifts is a linspace\n",
    "    logps = []\n",
    "\n",
    "    if shifts is not None:\n",
    "        for shift_value in shifts:\n",
    "            shift = torch.tensor(shift_value)\n",
    "            mu = x + shift\n",
    "            log_scale = torch.clamp(\n",
    "                torch.tensor(DEFAULT_LOGSCALE), min=-10.0, max=13.8155\n",
    "            )\n",
    "            scale = torch.exp(-0.5 * log_scale)\n",
    "            logp = discretized_logistic_logp(mu, scale, x, channel_ranges)\n",
    "            logps.append(-logp.item())\n",
    "        plt.plot(shifts, logps)\n",
    "        plt.xlabel(\"Shift\")\n",
    "        plt.ylabel(\"Negative Log Probability\")\n",
    "        plt.title(\"Negative Log Probability vs Shift\")\n",
    "        plt.show()\n",
    "    if logscales is not None:\n",
    "        logps = []\n",
    "        for logscale_value in logscales:\n",
    "            shift = torch.tensor(DEFAULT_SHIFT)\n",
    "            mu = x + shift\n",
    "            log_scale = torch.clamp(\n",
    "                torch.tensor(logscale_value), min=-10.0, max=13.8155\n",
    "            )\n",
    "            scale = torch.exp(-0.5 * log_scale)\n",
    "            logp = discretized_logistic_logp(mu, scale, x, channel_ranges)\n",
    "            logps.append(-logp.item())\n",
    "        plt.plot(logscales, logps)\n",
    "        plt.xlabel(\"Log Scale\")\n",
    "        plt.ylabel(\"Negative Log Probability\")\n",
    "        plt.title(\"Negative Log Probability vs Log Scale\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def logp_image(shifts: np.ndarray, logscales: np.ndarray):\n",
    "    channel_ranges = MonochromeBitdepths()\n",
    "    x = torch.ones((1, 1, 1, 1)) * DEFAULT_IMG_VALUE\n",
    "\n",
    "    logp_matrix = np.zeros((len(logscales), len(shifts)))\n",
    "\n",
    "    for j, shift_value in enumerate(shifts):\n",
    "        for i, logscale_value in enumerate(logscales):\n",
    "            shift = torch.tensor(shift_value)\n",
    "            mu = x + shift\n",
    "            log_scale = torch.clamp(\n",
    "                torch.tensor(logscale_value), min=-10.0, max=13.8155\n",
    "            )\n",
    "            scale = torch.exp(-0.5 * log_scale)\n",
    "            logp = discretized_logistic_logp(mu, scale, x, channel_ranges)\n",
    "            logp_matrix[i, j] = -logp.item()\n",
    "\n",
    "    plt.imshow(\n",
    "        logp_matrix,\n",
    "        extent=(shifts[0], shifts[-1], logscales[0], logscales[-1]),\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    plt.colorbar(label=\"Negative Log Probability\")\n",
    "    plt.xlabel(\"Shift\")\n",
    "    plt.ylabel(\"Log Scale\")\n",
    "    plt.title(\"Negative Log Probability Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "logp_image(np.linspace(-0.5, 0.5, 100), np.linspace(-10.0, 13.8154, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbb0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_out = torch.tensor(np.load(\"./testing/data/encoded_raw_out.npy\"))\n",
    "image = torch.tensor(np.load(\"./testing/data/original_image.npy\"))\n",
    "\n",
    "mu, scale = get_mu_and_scale_linear_color(\n",
    "    raw_out, image\n",
    ")\n",
    "print(np.mean(scale.numpy()))\n",
    "# scale = torch.ones_like(scale) * 0.022956606\n",
    "# mu = mu.clamp(0.0, 1.0)\n",
    "\n",
    "for channel in range(3):\n",
    "    channel_ranges = MonochromeBitdepths()\n",
    "    logps = - discretized_logistic_logp(\n",
    "        mu[:, channel : channel + 1],\n",
    "        scale[:, channel : channel + 1],\n",
    "        image[:, channel : channel + 1],\n",
    "        channel_ranges,\n",
    "    )\n",
    "    print((logps >= 10).sum())\n",
    "    print(logps.mean())\n",
    "    plt.figure()\n",
    "    plt.title(f\"Channel {channel} - logps values\")\n",
    "    plt.imshow(logps[0].permute(1, 2, 0).numpy(), vmin=0, vmax=16)\n",
    "    plt.colorbar(label=\"logp value\")\n",
    "    plt.xlabel(\"Width\")\n",
    "    plt.ylabel(\"Height\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image[0, 0].numpy(), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(image[0, 1].numpy(), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(image[0, 2].numpy(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c41a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in range(3):\n",
    "    channel_ranges = MonochromeBitdepths()\n",
    "    mu_slice = mu[:, channel : channel + 1]\n",
    "    image_slice = image[:, channel : channel + 1]\n",
    "    diff = abs(mu_slice - image_slice)\n",
    "    print(diff.min(), diff.mean(), diff.max())\n",
    "    plt.figure()\n",
    "    plt.title(f\"Channel {channel} - mu values\")\n",
    "    plt.imshow(diff[0].permute(1, 2, 0).numpy()*255)\n",
    "    plt.colorbar(label=\"mu value\")\n",
    "    plt.xlabel(\"Width\")\n",
    "    plt.ylabel(\"Height\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e983593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchac\n",
    "from lossless.util.distribution import compute_logistic_cdfs\n",
    "from typing import Literal\n",
    "from lossless.util.color_transform import ColorBitdepths\n",
    "\n",
    "POSSIBLE_DISTRIBUTIONS = Literal[\"logistic\", \"laplace\"]\n",
    "\n",
    "\n",
    "def get_bits_per_pixel(w, h, c, encoded_bytes):\n",
    "    num_pixels = w * h * c\n",
    "    num_bits = 0\n",
    "    for bytes_channel in encoded_bytes:\n",
    "        num_bits += len(bytes_channel) * 8\n",
    "    return num_bits / num_pixels\n",
    "\n",
    "\n",
    "def _laplace_cdf(\n",
    "    x: torch.Tensor, expectation: torch.Tensor, scale: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the laplace cumulative evaluated in x. All parameters\n",
    "    must have the same dimension.\n",
    "    Re-implemented here coz it is faster than calling the Laplace distribution\n",
    "    from torch.distributions.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Where the cumulative if evaluated.\n",
    "        expectation (Tensor): Expectation.\n",
    "        scale (Tensor): Scale\n",
    "\n",
    "    Returns:\n",
    "        Tensor: CDF(x, mu, scale)\n",
    "    \"\"\"\n",
    "    shifted_x = x - expectation\n",
    "    return 0.5 - 0.5 * (shifted_x).sign() * torch.expm1(\n",
    "        -(shifted_x).abs() / scale\n",
    "    )\n",
    "\n",
    "\n",
    "def _logistic_cdf(\n",
    "    x: torch.Tensor, mu: torch.Tensor, s: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the logistic cumulative evaluated in x. All parameters\n",
    "    must have the same dimension.\n",
    "    Re-implemented here coz it is faster than calling the Logistic distribution\n",
    "    from torch.distributions.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Where the cumulative if evaluated.\n",
    "        mu (Tensor): Expectation.\n",
    "        s (Tensor): Scale\n",
    "\n",
    "    Returns:\n",
    "        Tensor: CDF(x, mu, scale)\n",
    "    \"\"\"\n",
    "    z = (x - mu) / s\n",
    "    return torch.sigmoid(z)\n",
    "\n",
    "\n",
    "def calculate_probability_distribution(\n",
    "    mu,\n",
    "    s,\n",
    "    color_bitdepths: ColorBitdepths,\n",
    "    distribution: POSSIBLE_DISTRIBUTIONS,\n",
    "    channel_idx: int,\n",
    "):\n",
    "    \"\"\"Calculate Logistic probability distribution for arithmetic coding.\n",
    "    Works for any shape of mu and s, adds one dimension at the end for the probability axis.\n",
    "    \"\"\"\n",
    "    # Create the base tensor of quantized values\n",
    "    new_tensor = torch.linspace(\n",
    "        color_bitdepths.ranges_int[channel_idx][0]\n",
    "        / color_bitdepths.scaling_factors[channel_idx],\n",
    "        color_bitdepths.ranges_int[channel_idx][1]\n",
    "        / color_bitdepths.scaling_factors[channel_idx],\n",
    "        steps=color_bitdepths.bins[channel_idx],\n",
    "        device=mu.device,\n",
    "    )\n",
    "    new_shape = (\n",
    "        *mu.shape,\n",
    "        color_bitdepths.bins[channel_idx],\n",
    "    )  # add one dimension at the end\n",
    "    new_tensor = new_tensor.view(\n",
    "        *([1] * mu.ndim), color_bitdepths.bins[channel_idx]\n",
    "    ).expand(*new_shape)\n",
    "\n",
    "    # Compute boundaries for each bin\n",
    "    x_minus = new_tensor - 0.5 / color_bitdepths.bins[channel_idx]\n",
    "    x_plus = new_tensor + 0.5 / color_bitdepths.bins[channel_idx]\n",
    "\n",
    "    # Expand mu and s with one trailing dimension\n",
    "    mu_expanded = mu.unsqueeze(-1)\n",
    "    s_expanded = s.unsqueeze(-1)\n",
    "\n",
    "    # Logistic CDF difference between bin edges (use the logistic cdf function)\n",
    "    if distribution == \"laplace\":\n",
    "        cdf_minus = _laplace_cdf(x_minus, mu_expanded, s_expanded)\n",
    "        cdf_plus = _laplace_cdf(x_plus, mu_expanded, s_expanded)\n",
    "    elif distribution == \"logistic\":\n",
    "        cdf_minus = _logistic_cdf(x_minus, mu_expanded, s_expanded)\n",
    "        cdf_plus = _logistic_cdf(x_plus, mu_expanded, s_expanded)\n",
    "\n",
    "    prob_t = cdf_plus - cdf_minus\n",
    "    prob_t = torch.clamp_min(prob_t, 2 ** (-16))\n",
    "    prob_t = prob_t / prob_t.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    return prob_t\n",
    "\n",
    "\n",
    "def dist_to_cfd(prob_dist: torch.Tensor) -> torch.Tensor:\n",
    "    # we go from probability table in the last dimension [..., num_symbols] to CDF table in the last dimension [..., num_symbols + 1]\n",
    "    cdf = torch.zeros(\n",
    "        *prob_dist.shape[:-1],\n",
    "        prob_dist.shape[-1] + 1,\n",
    "        device=prob_dist.device,\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    cdf[..., 1:] = prob_dist.cumsum(dim=-1)\n",
    "    cdf[..., -1] = 0  # ensure last value is exactly 0\n",
    "    cdf = torch.round(cdf * float(1 << 16)).to(torch.int32)\n",
    "    cdf[..., -1] = 0  # ensure last value is exactly 2^16\n",
    "    return cdf.to(torch.int16)\n",
    "\n",
    "\n",
    "def encode(\n",
    "    x: torch.Tensor,\n",
    "    mu: torch.Tensor,\n",
    "    scale: torch.Tensor,\n",
    "    color_bitdepths: ColorBitdepths,\n",
    "    distribution: POSSIBLE_DISTRIBUTIONS = \"logistic\",\n",
    "):\n",
    "    # this undoes normalization\n",
    "    x_reshape = torch.floor(x * 255).to(torch.int16).cpu()\n",
    "\n",
    "    byte_strings = []\n",
    "    for i in range(3):\n",
    "        symbols = x_reshape[:, i : i + 1, ...]\n",
    "        # print(\n",
    "        #     f\"Channel {i}: symbols shape {symbols.shape}, min {symbols.min()}, max {symbols.max()}\"\n",
    "        # )\n",
    "        cur_cdfs = dist_to_cfd(\n",
    "            calculate_probability_distribution(\n",
    "                mu[:, i : i + 1, ...],\n",
    "                scale[:, i : i + 1, ...],\n",
    "                color_bitdepths=color_bitdepths,\n",
    "                distribution=distribution,\n",
    "                channel_idx=i,\n",
    "            )\n",
    "        ).cpu()\n",
    "        byte_strings.append(\n",
    "            torchac.encode_int16_normalized_cdf(cur_cdfs, symbols)\n",
    "        )\n",
    "    return byte_strings\n",
    "\n",
    "\n",
    "def decode(\n",
    "    byte_strings: list,\n",
    "    mu: torch.Tensor,\n",
    "    scale: torch.Tensor,\n",
    "    color_bitdepths: ColorBitdepths,\n",
    "    distribution: POSSIBLE_DISTRIBUTIONS = \"logistic\",\n",
    "):\n",
    "    assert len(byte_strings) == 3\n",
    "\n",
    "    _, _, h, w = mu.size()\n",
    "    x_rec = torch.zeros(1, 3, h, w)\n",
    "\n",
    "    # Channel 0 (Red)\n",
    "    cur_cdfs_r = dist_to_cfd(\n",
    "        calculate_probability_distribution(\n",
    "            mu[:, :1, ...],\n",
    "            scale[:, :1, ...],\n",
    "            color_bitdepths=color_bitdepths,\n",
    "            distribution=distribution,\n",
    "            channel_idx=0,\n",
    "        )\n",
    "    ).cpu()\n",
    "    symbols_r = torchac.decode_int16_normalized_cdf(cur_cdfs_r, byte_strings[0])\n",
    "    print(\n",
    "        f\"Decoded R: shape {symbols_r.shape}, min {symbols_r.min()}, max {symbols_r.max()}\"\n",
    "    )\n",
    "    x_r = symbols_r.reshape(1, 1, h, w).float() / 255\n",
    "    x_rec[:, 0:1, ...] = x_r  # FIX: was using 3*i (which is 0), should be 0:1\n",
    "\n",
    "    # Channel 1 (Green)\n",
    "    cur_cdfs_g = dist_to_cfd(\n",
    "        calculate_probability_distribution(\n",
    "            mu[:, 1:2, ...],\n",
    "            scale[:, 1:2, ...],\n",
    "            color_bitdepths=color_bitdepths,\n",
    "            distribution=distribution,\n",
    "            channel_idx=1,\n",
    "        )\n",
    "    ).cpu()\n",
    "    symbols_g = torchac.decode_int16_normalized_cdf(cur_cdfs_g, byte_strings[1])\n",
    "    print(\n",
    "        f\"Decoded G: shape {symbols_g.shape}, min {symbols_g.min()}, max {symbols_g.max()}\"\n",
    "    )\n",
    "    x_g = symbols_g.reshape(1, 1, h, w).float() / 255\n",
    "    x_rec[:, 1:2, ...] = x_g  # FIX: was using 3*i+1 (which is 1), should be 1:2\n",
    "\n",
    "    # Channel 2 (Blue)\n",
    "    cur_cdfs_b = dist_to_cfd(\n",
    "        calculate_probability_distribution(\n",
    "            mu[:, 2:3, ...],\n",
    "            scale[:, 2:3, ...],\n",
    "            color_bitdepths=color_bitdepths,\n",
    "            distribution=distribution,\n",
    "            channel_idx=2,\n",
    "        )\n",
    "    ).cpu()\n",
    "    symbols_b = torchac.decode_int16_normalized_cdf(cur_cdfs_b, byte_strings[2])\n",
    "    print(\n",
    "        f\"Decoded B: shape {symbols_b.shape}, min {symbols_b.min()}, max {symbols_b.max()}\"\n",
    "    )\n",
    "    x_b = symbols_b.reshape(1, 1, h, w).float() / 255\n",
    "    x_rec[:, 2:3, ...] = x_b  # FIX: was using 3*i+2 (which is 2), should be 2:3\n",
    "\n",
    "    return x_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fdf2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color_bitdepths = color_transform.YCoCgBitdepths()\n",
    "# prob_logistic = calculate_probability_distribution(mu, scale, color_bitdepths, distribution=\"logistic\", channel_idx=1)[0]\n",
    "# print(prob_logistic.shape)\n",
    "# # prob_logistic has shape [3, 512, 768, 256]\n",
    "# # image has shape [3, 512, 768]\n",
    "# # index into prob_logistic using image values\n",
    "# # to get the probabilities of the actual image values\n",
    "# indices = (image[0]*255).long()\n",
    "# indices = indices.clamp(0, 255) - color_bitdepths.ranges_int[1][0]\n",
    "# # indices = torch.zeros_like(indices)\n",
    "# gathered_probs = prob_logistic.permute(0, 3, 1, 2).gather(1, indices.unsqueeze(1)).squeeze(1)\n",
    "# print(gathered_probs.shape)  # should be [3, 512, 768]\n",
    "\n",
    "# for channel in range(3):\n",
    "#     channel_ranges = MonochromeBitdepths()\n",
    "#     gp = gathered_probs[channel : channel + 1]\n",
    "#     print(gp.mean(), gp.shape)\n",
    "#     plt.figure()\n",
    "#     plt.title(f\"Channel {channel} - probs values\")\n",
    "#     plt.imshow(gp[0].numpy())\n",
    "#     plt.colorbar(label=\"probs value\")\n",
    "#     plt.xlabel(\"Width\")\n",
    "#     plt.ylabel(\"Height\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f6ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchac\n",
    "from lossless.util.distribution import compute_logistic_cdfs\n",
    "from typing import Literal\n",
    "from lossless.util.color_transform import ColorBitdepths\n",
    "\n",
    "POSSIBLE_DISTRIBUTIONS = Literal[\"logistic\", \"laplace\", \"dummy\"]\n",
    "\n",
    "\n",
    "def get_bits_per_pixel(w, h, c, encoded_bytes):\n",
    "    num_pixels = w * h * c\n",
    "    num_bits = 0\n",
    "    for bytes_channel in encoded_bytes:\n",
    "        num_bits += len(bytes_channel) * 8\n",
    "    return num_bits / num_pixels\n",
    "\n",
    "\n",
    "def _laplace_cdf(\n",
    "    x: torch.Tensor, expectation: torch.Tensor, scale: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the laplace cumulative evaluated in x. All parameters\n",
    "    must have the same dimension.\n",
    "    Re-implemented here coz it is faster than calling the Laplace distribution\n",
    "    from torch.distributions.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Where the cumulative if evaluated.\n",
    "        expectation (Tensor): Expectation.\n",
    "        scale (Tensor): Scale\n",
    "\n",
    "    Returns:\n",
    "        Tensor: CDF(x, mu, scale)\n",
    "    \"\"\"\n",
    "    shifted_x = x - expectation\n",
    "    return 0.5 - 0.5 * (shifted_x).sign() * torch.expm1(\n",
    "        -(shifted_x).abs() / scale\n",
    "    )\n",
    "\n",
    "\n",
    "def _logistic_cdf(\n",
    "    x: torch.Tensor, mu: torch.Tensor, s: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the logistic cumulative evaluated in x. All parameters\n",
    "    must have the same dimension.\n",
    "    Re-implemented here coz it is faster than calling the Logistic distribution\n",
    "    from torch.distributions.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Where the cumulative if evaluated.\n",
    "        mu (Tensor): Expectation.\n",
    "        s (Tensor): Scale\n",
    "\n",
    "    Returns:\n",
    "        Tensor: CDF(x, mu, scale)\n",
    "    \"\"\"\n",
    "    z = (x - mu) / s\n",
    "    return torch.sigmoid(z)\n",
    "\n",
    "def calculate_probability_distribution(\n",
    "    mu,\n",
    "    s,\n",
    "    color_bitdepths: ColorBitdepths,\n",
    "    distribution: POSSIBLE_DISTRIBUTIONS,\n",
    "    channel_idx: int,\n",
    "):\n",
    "    \"\"\"Calculate Logistic probability distribution for arithmetic coding.\n",
    "    Works for any shape of mu and s, adds one dimension at the end for the probability axis.\n",
    "    \"\"\"\n",
    "    # Create the base tensor of quantized values\n",
    "    new_tensor = torch.linspace(\n",
    "        0.0,\n",
    "        (\n",
    "            color_bitdepths.ranges_int[channel_idx][1]\n",
    "            - color_bitdepths.ranges_int[channel_idx][0]\n",
    "        )\n",
    "        / color_bitdepths.scaling_factors[channel_idx],\n",
    "        steps=color_bitdepths.bins[channel_idx],\n",
    "        device=mu.device,\n",
    "    )\n",
    "    new_shape = (\n",
    "        *mu.shape,\n",
    "        color_bitdepths.bins[channel_idx],\n",
    "    )  # add one dimension at the end\n",
    "    new_tensor = new_tensor.view(\n",
    "        *([1] * mu.ndim), color_bitdepths.bins[channel_idx]\n",
    "    ).expand(*new_shape)\n",
    "\n",
    "    # Compute boundaries for each bin\n",
    "    x_minus = new_tensor - 0.5 / color_bitdepths.bins[channel_idx]\n",
    "    x_plus = new_tensor + 0.5 / color_bitdepths.bins[channel_idx]\n",
    "\n",
    "    # Expand mu and s with one trailing dimension\n",
    "    mu_expanded = mu.unsqueeze(-1)\n",
    "    s_expanded = s.unsqueeze(-1)\n",
    "\n",
    "    # Logistic CDF difference between bin edges (use the logistic cdf function)\n",
    "    if distribution == \"laplace\":\n",
    "        cdf_minus = _laplace_cdf(x_minus, mu_expanded, s_expanded)\n",
    "        cdf_plus = _laplace_cdf(x_plus, mu_expanded, s_expanded)\n",
    "    else:\n",
    "        cdf_minus = _logistic_cdf(x_minus, mu_expanded, s_expanded)\n",
    "        cdf_plus = _logistic_cdf(x_plus, mu_expanded, s_expanded)\n",
    "    prob_t = cdf_plus - cdf_minus\n",
    "    if distribution == \"dummy\":\n",
    "        prob_t = torch.ones_like(cdf_plus)\n",
    "    prob_t = torch.clamp_min(prob_t, 2 ** (-16))\n",
    "    prob_t = prob_t / prob_t.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    assert torch.all(torch.isclose(prob_t.sum(dim=-1), torch.ones_like(mu))), \"Probabilities do not sum to 1\"\n",
    "    assert torch.all(prob_t.min() > 0) , \"Some probabilities are zero\"\n",
    "    return prob_t\n",
    "\n",
    "\n",
    "def dist_to_cfd(prob_dist: torch.Tensor) -> torch.Tensor:\n",
    "    # we go from probability table in the last dimension [..., num_symbols] to CDF table in the last dimension [..., num_symbols + 1]\n",
    "    cdf = torch.zeros(\n",
    "        *prob_dist.shape[:-1],\n",
    "        prob_dist.shape[-1] + 1,\n",
    "        device=prob_dist.device,\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    cdf[..., 1:] = prob_dist.cumsum(dim=-1)\n",
    "    cdf = torch.round(cdf * float(1 << 16))\n",
    "    cdf[..., -1] = 0\n",
    "    return cdf.to(torch.int16)\n",
    "\n",
    "\n",
    "# def encode(\n",
    "#     x: torch.Tensor,\n",
    "#     mu: torch.Tensor,\n",
    "#     scale: torch.Tensor,\n",
    "#     color_bitdepths: ColorBitdepths,\n",
    "#     distribution: POSSIBLE_DISTRIBUTIONS = \"logistic\",\n",
    "# ):\n",
    "#     # this undoes normalization\n",
    "#     x_reshape = torch.floor(x * 255)\n",
    "\n",
    "#     byte_strings = []\n",
    "#     for i in range(3):\n",
    "#         symbols = x_reshape[:, i : i + 1, ...]\n",
    "#         print(\n",
    "#             f\"Channel {i}: symbols shape {symbols.shape}, min {symbols.min()}, max {symbols.max()}\"\n",
    "#         )\n",
    "#         symbols = symbols - color_bitdepths.ranges_int[i][0]\n",
    "#         mu_print = mu - (color_bitdepths.ranges_int[i][0] / color_bitdepths.scaling_factors[i])\n",
    "\n",
    "#         cur_cdfs = dist_to_cfd(\n",
    "#             calculate_probability_distribution(\n",
    "#                 mu_print[:, i : i + 1, ...],\n",
    "#                 scale[:, i : i + 1, ...],\n",
    "#                 color_bitdepths=color_bitdepths,\n",
    "#                 distribution=distribution,\n",
    "#                 channel_idx=i,\n",
    "#             )\n",
    "#         ).cpu()\n",
    "#         byte_strings.append(\n",
    "#             torchac.encode_int16_normalized_cdf(\n",
    "#                 cur_cdfs, symbols.to(torch.int16).cpu()\n",
    "#             )\n",
    "#         )\n",
    "#     return byte_strings\n",
    "\n",
    "\n",
    "# def decode(\n",
    "#     byte_strings: list,\n",
    "#     mu: torch.Tensor,\n",
    "#     scale: torch.Tensor,\n",
    "#     color_bitdepths: ColorBitdepths,\n",
    "#     distribution: POSSIBLE_DISTRIBUTIONS = \"logistic\",\n",
    "# ):\n",
    "#     assert len(byte_strings) == 3\n",
    "\n",
    "#     _, _, h, w = mu.size()\n",
    "#     x_rec = torch.zeros(1, 3, h, w)\n",
    "\n",
    "#     for i in range(3):\n",
    "\n",
    "#         # Channel 0 (Red)\n",
    "#         cur_cdfs = dist_to_cfd(\n",
    "#             calculate_probability_distribution(\n",
    "#                 mu[:, i : i + 1, ...]\n",
    "#                 - (color_bitdepths.ranges_int[i][0]\n",
    "#                 / color_bitdepths.scaling_factors[i]),\n",
    "#                 scale[:, i : i + 1, ...],\n",
    "#                 color_bitdepths=color_bitdepths,\n",
    "#                 distribution=distribution,\n",
    "#                 channel_idx=i,\n",
    "#             )\n",
    "#         ).cpu()\n",
    "#         symbols = (\n",
    "#             torchac.decode_int16_normalized_cdf(cur_cdfs, byte_strings[0])\n",
    "#             + color_bitdepths.ranges_int[i][0]\n",
    "#         )\n",
    "#         print(\n",
    "#             f\"Decoded R: shape {symbols.shape}, min {symbols.min()}, max {symbols.max()}\"\n",
    "#         )\n",
    "#         x = symbols.reshape(1, 1, h, w).float() / 255\n",
    "#         x_rec[:, i : i + 1, ...] = (\n",
    "#             x  # FIX: was using 3*i (which is 0), should be 0:1\n",
    "#         )\n",
    "\n",
    "#     return x_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import constriction\n",
    "import struct\n",
    "\n",
    "\n",
    "def encode(\n",
    "    x: torch.Tensor,\n",
    "    mu: torch.Tensor,\n",
    "    scale: torch.Tensor,\n",
    "    ct: color_transform.ColorBitdepths = color_transform.YCoCgBitdepths(),\n",
    "    output_path=\"./test-workdir/encoder_size_test/coolchic_encoded.binary\",\n",
    "):\n",
    "    x = x * 255.0\n",
    "    # print(x[0, :, :5, :5])\n",
    "    x_reshape = x.to(torch.int16).cpu()\n",
    "\n",
    "    B, C, H, W = x.shape\n",
    "    enc = constriction.stream.stack.AnsCoder()\n",
    "    bits_theoretical = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scale_theoretical_bits = 0\n",
    "        mu = mu.flatten(2, 3).permute(0, 2, 1)  # B, H*W, C\n",
    "        scale = scale.flatten(2, 3).permute(0, 2, 1)  # B, H*W, C\n",
    "\n",
    "        probs_logistic = [\n",
    "            calculate_probability_distribution(\n",
    "                mu - ct.ranges_int[ch_ind][0] / ct.scaling_factors[ch_ind],\n",
    "                scale,\n",
    "                color_bitdepths=ct,\n",
    "                distribution=\"logistic\",\n",
    "                channel_idx=ch_ind,\n",
    "            )\n",
    "            for ch_ind in range(C)\n",
    "        ]\n",
    "\n",
    "        scale_theoretical_bits = 0\n",
    "        for wh in range(x.shape[2] * x.shape[3]):\n",
    "            for c in range(x.shape[1]):\n",
    "                sym: int = (\n",
    "                    x_reshape.flatten(2, 3).permute(0, 2, 1)[0, -wh - 1, -c - 1]\n",
    "                ).int().item() - ct.ranges_int[-c - 1][0]\n",
    "                prob_t = probs_logistic[-c - 1][0, -wh - 1, -c - 1]\n",
    "                scale_theoretical_bits += -torch.log2(prob_t[sym]).item()\n",
    "                model = constriction.stream.model.Categorical(\n",
    "                    prob_t.detach().cpu().numpy(), perfect=False\n",
    "                )\n",
    "                try:\n",
    "                    enc.encode_reverse(sym, model)\n",
    "                except Exception as e:\n",
    "\n",
    "                    print(f\"Error encoding symbol {sym} for channel {c}: {e}\")\n",
    "                    raise e\n",
    "        bits_theoretical += scale_theoretical_bits\n",
    "\n",
    "    bitstream = enc.get_compressed()\n",
    "    bitstream.tofile(output_path)\n",
    "    with open(output_path, \"rb\") as f:\n",
    "        original_data = f.read()\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        # Pack two 32-bit integers into binary\n",
    "        f.write(struct.pack(\"iii\", H, W, C))\n",
    "        f.write(original_data)\n",
    "\n",
    "    print(f\"Theoretical bits per sub pixel: {bits_theoretical/float(W*H*C)}\")\n",
    "    return probs_logistic\n",
    "\n",
    "\n",
    "def decode(bitstream_path, mu: torch.Tensor, scale: torch.Tensor, ct=color_transform.ColorBitdepths()):\n",
    "    with open(bitstream_path, \"rb\") as f:\n",
    "        header = f.read(12)  # 3 integers * 4 bytes each\n",
    "        H, W, C = struct.unpack(\"iii\", header)\n",
    "    bitstream = np.fromfile(bitstream_path, dtype=np.uint32, offset=12)\n",
    "    dec = constriction.stream.stack.AnsCoder(bitstream)\n",
    "\n",
    "    x = -torch.ones(1, C, H, W)\n",
    "    with torch.no_grad():\n",
    "        mu = mu.flatten(2, 3).permute(0, 2, 1)  # B, H*W, C\n",
    "        scale = scale.flatten(2, 3).permute(0, 2, 1)  # B, H*W, C\n",
    "\n",
    "        probs_logistic = [\n",
    "            calculate_probability_distribution(\n",
    "                mu - ct.ranges_int[ch_ind][0] / ct.scaling_factors[ch_ind],\n",
    "                scale,\n",
    "                color_bitdepths=ct,\n",
    "                distribution=\"logistic\",\n",
    "                channel_idx=ch_ind,\n",
    "            )\n",
    "            for ch_ind in range(C)\n",
    "        ]\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "                for c in range(C):\n",
    "                    prob = probs_logistic[c][0, h * W + w, c]\n",
    "                    prob_array = prob.detach().cpu().flatten().numpy()\n",
    "                    model = constriction.stream.model.Categorical(\n",
    "                        prob_array, perfect=False\n",
    "                    )\n",
    "                    decoded_char = (\n",
    "                        torch.tensor(dec.decode(model, 1)[0]).float()\n",
    "                        + ct.ranges_int[c][0]\n",
    "                    )\n",
    "                    x[0, c, h, w] = decoded_char\n",
    "\n",
    "    # print(x[0, :, :5, :5])\n",
    "    x = x.cpu() / 255\n",
    "\n",
    "    return x, probs_logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56402e41",
   "metadata": {},
   "source": [
    "# TESTING ENCODE DECODE LOSSLESSNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5cf815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = torch.ones((1, 3, 32, 32)) * 0.5\n",
    "# image[:, 0, :16, :16] = 0.1\n",
    "# image[:, 1, :16, 16:] = 0.3\n",
    "# image[:, 2, 16:, :16] = 0.9\n",
    "image = torch.rand((1, 3, 32, 32))\n",
    "image[0, 1:] = image[0, 1:] * 2 - 1\n",
    "image = torch.round(image * 255) / 255\n",
    "\n",
    "mu = torch.rand_like(image)\n",
    "scale = torch.rand_like(image)\n",
    "\n",
    "col_b = color_transform.YCoCgBitdepths()\n",
    "probs_enc = encode(image, mu, scale, ct=col_b)\n",
    "decoded_image, probs_dec = decode(\"./test-workdir/encoder_size_test/coolchic_encoded.binary\", mu, scale, ct=col_b)\n",
    "\n",
    "assert torch.allclose(probs_enc[0], probs_dec[0]), \"Probability distributions for channel 0 do not match after encoding and decoding\"\n",
    "\n",
    "# show the original and decoded images side by side. Show the channels separately in a 3 rows and 3 columns layout\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 9))\n",
    "diff = torch.abs(image - decoded_image)\n",
    "channel_names = ['Red', 'Green', 'Blue']\n",
    "for i in range(3):\n",
    "    axs[i, 0].imshow(image[0, i].cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "    axs[i, 0].set_title(f'Original {channel_names[i]} Channel min {image[0, i].min().item():.2f} max {image[0, i].max().item():.2f}')\n",
    "    axs[i, 0].axis('off')\n",
    "\n",
    "    axs[i, 1].imshow(decoded_image[0, i].cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "    axs[i, 1].set_title(f'Decoded {channel_names[i]} Channel min {decoded_image[0, i].min().item():.2f} max {decoded_image[0, i].max().item():.2f}')\n",
    "    axs[i, 1].axis('off')\n",
    "\n",
    "    axs[i, 2].imshow(diff[0, i].cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "    axs[i, 2].set_title(f'Difference {channel_names[i]} Channel min {diff[0, i].min().item():.2f} max {diff[0, i].max().item():.2f}')\n",
    "    axs[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f\"Max difference between original and decoded image: {diff.max().item()}\")\n",
    "# show diff channel by channel\n",
    "assert torch.allclose(image, decoded_image, atol=1e-6)\n",
    "print(\"Encode-decode losslessness test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3101548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, scale = get_mu_and_scale_linear_color(raw_out, image)\n",
    "\n",
    "# mu = mu.clamp(0.0, 0.0)\n",
    "\n",
    "enc = encode(image, mu, scale, output_path=\"./test-workdir/encoder_size_test/coolchic_encoded_dummy.binary\")\n",
    "# enc = encode(image, mu, scale, color_bitdepths=color_transform.YCoCgBitdepths(), distribution=\"dummy\")\n",
    "# # bpp = get_bits_per_pixel(1.0, 1.0, 1.0, enc) / image.numel()\n",
    "# # print(f\"Image bpd: {bpp}\")\n",
    "# dec = decode(enc, mu, scale, color_bitdepths=color_transform.YCoCgBitdepths(), distribution=\"dummy\")\n",
    "\n",
    "# # print(dec.shape)\n",
    "# # print(image.shape)\n",
    "# diff = torch.abs(image.cpu()*255 - dec * 255)\n",
    "# print(f\"Max difference after decoding: {diff.max().item()}\")\n",
    "\n",
    "# print(\"Red channel difference:\", diff[0,0].min(), diff[0,0].max())\n",
    "# print(\"Green channel difference:\", diff[0,1].min(), diff[0,1].max())\n",
    "# print(\"Blue channel difference:\", diff[0,2].min(), diff[0,2].max())\n",
    "# plt.imshow(diff[0,0].cpu().numpy() * 100)\n",
    "# plt.show()\n",
    "# plt.imshow(diff[0,1].cpu().numpy() * 100)\n",
    "# plt.show()\n",
    "# plt.imshow(diff[0,2].cpu().numpy() * 100)\n",
    "# plt.show()\n",
    "\n",
    "# # assert torch.allclose(\n",
    "# #     image.cpu(), dec.cpu()\n",
    "# # ), \"Decoded image does not match original!\"\n",
    "# # print(\"Decoded image matches original!\")\n",
    "\n",
    "# print(image.numel())\n",
    "# bpp = get_bits_per_pixel(1.0, 1.0, 1.0, enc) / image.numel()\n",
    "# print(f\"Image bpd: {bpp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c631bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = decode(\n",
    "    \"./test-workdir/encoder_size_test/coolchic_encoded.binary\",\n",
    "    mu,\n",
    "    scale,\n",
    "    output_path=\"./test-workdir/encoder_size_test/coolchic_decoded.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(\n",
    "    image.cpu(), torch.from_numpy(dec).cpu()\n",
    "), \"Decoded image does not match original!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d983db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for C in range(3):\n",
    "    plt.imshow(dec[0, C, :, :], cmap='gray')\n",
    "    plt.title(f\"Decoded channel {C}\")\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cool_chic_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
