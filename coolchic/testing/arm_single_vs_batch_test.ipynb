{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52b7c3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "torch.Size([10, 16])\n",
      "torch.Size([10, 16])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"testing\":\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "import lossless.component.core.arm as arm_core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"]=\"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]=\"1\"\n",
    "torch.set_num_threads(1)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "arm_model = arm_core.Arm(dim_arm=16, n_hidden_layers_arm=2).to(DEVICE)\n",
    "# load pretrained weights\n",
    "arm_model.load_state_dict(torch.load(\"../logs/full_runs/trained_models/coolchic_arm.pth\", map_location=DEVICE))\n",
    "arm_model.eval()\n",
    "\n",
    "# shape B,C,H,W\n",
    "latents_dict = torch.load(\"../logs/full_runs/trained_models/coolchic_latents_snapshot.pt\", map_location=DEVICE)\n",
    "flat_latent = latents_dict[\"flat_latent\"].to(DEVICE)\n",
    "latent_context_flat = latents_dict[\"latent_context_flat\"].to(DEVICE)\n",
    "print(flat_latent.shape)\n",
    "print(latent_context_flat.shape)\n",
    "latents = torch.zeros((1,1,10,10), dtype=torch.float32).to(DEVICE)\n",
    "latent_context_flat = torch.concat([arm_model.get_neighbor_context(latents[0,0].tolist(), i, j) for j in range(latents.shape[3]) for i in range(latents.shape[2])])[:10]\n",
    "print(latent_context_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2342ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from lossless.util.misc import safe_get_from_nested_lists\n",
    "from torch import Tensor, index_select, nn\n",
    "\n",
    "\n",
    "class ArmLinear(nn.Module):\n",
    "    \"\"\"Create a Linear layer of the Auto-Regressive Module (ARM). This is a\n",
    "    wrapper around the usual ``nn.Linear`` layer of PyTorch, with a custom\n",
    "    initialization. It performs the following operations:\n",
    "\n",
    "    * :math:`\\\\mathbf{x}_{out} = \\\\mathbf{W}\\\\mathbf{x}_{in} + \\\\mathbf{b}` if\n",
    "      ``residual`` is ``False``\n",
    "\n",
    "    * :math:`\\\\mathbf{x}_{out} = \\\\mathbf{W}\\\\mathbf{x}_{in} + \\\\mathbf{b} +\n",
    "      \\\\mathbf{x}_{in}` if ``residual`` is ``True``.\n",
    "\n",
    "    The input  :math:`\\\\mathbf{x}_{in}` is a :math:`[B, C_{in}]` tensor, the\n",
    "    output :math:`\\\\mathbf{x}_{out}` is a :math:`[B, C_{out}]` tensor.\n",
    "\n",
    "    The layer weight and bias shapes are :math:`\\\\mathbf{W} \\\\in\n",
    "    \\\\mathbb{R}^{C_{out} \\\\times C_{in}}` and :math:`\\\\mathbf{b} \\\\in\n",
    "    \\\\mathbb{R}^{C_{out}}`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        residual: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Number of input features :math:`C_{in}`.\n",
    "            out_channels: Number of output features :math:`C_{out}`.\n",
    "            residual: True to add a residual connection to the layer. Defaults to\n",
    "                False.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = residual\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # -------- Instantiate empty parameters, set by the initialize function\n",
    "        self.weight = nn.Parameter(torch.empty(out_channels, in_channels), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.empty((out_channels)), requires_grad=True)\n",
    "        self.initialize_parameters()\n",
    "        # -------- Instantiate empty parameters, set by the initialize function\n",
    "\n",
    "    def initialize_parameters(self) -> None:\n",
    "        \"\"\"Initialize **in place** the weight and the bias of the linear layer.\n",
    "\n",
    "        * Biases are always set to zero.\n",
    "\n",
    "        * Weights are set to zero if ``residual == True``. Otherwise, sample\n",
    "          from the Normal distribution: :math:`\\\\mathbf{W} \\sim \\\\mathcal{N}(0,\n",
    "          \\\\tfrac{1}{(C_{out})^4})`.\n",
    "        \"\"\"\n",
    "        self.bias = nn.Parameter(torch.zeros_like(self.bias), requires_grad=True)\n",
    "        if self.residual:\n",
    "            self.weight = nn.Parameter(torch.zeros_like(self.weight), requires_grad=True)\n",
    "        else:\n",
    "            out_channel = self.weight.size()[0]\n",
    "            self.weight = nn.Parameter(\n",
    "                torch.randn_like(self.weight) / out_channel**2, requires_grad=True\n",
    "            )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Perform the forward pass of this layer.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape :math:`[B, C_{in}]`.\n",
    "\n",
    "        Returns:\n",
    "            Tensor with shape :math:`[B, C_{out}]`.\n",
    "        \"\"\"\n",
    "        if self.residual:\n",
    "            return F.linear(x, self.weight, bias=self.bias) + x\n",
    "\n",
    "        # Not residual\n",
    "        else:\n",
    "            return F.linear(x, self.weight, bias=self.bias)\n",
    "\n",
    "\n",
    "class Arm(nn.Module):\n",
    "    \"\"\"Instantiate an autoregressive probability module, modelling the\n",
    "    conditional distribution :math:`p_{\\\\psi}(\\\\hat{y}_i \\\\mid\n",
    "    \\\\mathbf{c}_i)` of a (quantized) latent pixel :math:`\\\\hat{y}_i`,\n",
    "    conditioned on neighboring already decoded context pixels\n",
    "    :math:`\\\\mathbf{c}_i \\in \\\\mathbb{Z}^C`, where :math:`C` denotes the\n",
    "    number of context pixels.\n",
    "\n",
    "    The distribution :math:`p_{\\\\psi}` is assumed to follow a Laplace\n",
    "    distribution, parameterized by an expectation :math:`\\\\mu` and a scale\n",
    "    :math:`b`, where the scale and the variance :math:`\\\\sigma^2` are\n",
    "    related as follows :math:`\\\\sigma^2 = 2 b ^2`.\n",
    "\n",
    "    The parameters of the Laplace distribution for a given latent pixel\n",
    "    :math:`\\\\hat{y}_i` are obtained by passing its context pixels\n",
    "    :math:`\\\\mathbf{c}_i` through an MLP :math:`f_{\\\\psi}`:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        p_{\\\\psi}(\\\\hat{y}_i \\\\mid \\\\mathbf{c}_i) \\sim \\mathcal{L}(\\\\mu_i,\n",
    "        b_i), \\\\text{ where } \\\\mu_i, b_i = f_{\\\\psi}(\\\\mathbf{c}_i).\n",
    "\n",
    "    .. attention::\n",
    "\n",
    "        The MLP :math:`f_{\\\\psi}` has a few constraint on its architecture:\n",
    "\n",
    "        * The width of all hidden layers (i.e. the output of all layers except\n",
    "          the final one) are identical to the number of pixel contexts\n",
    "          :math:`C`;\n",
    "\n",
    "        * All layers except the last one are residual layers, followed by a\n",
    "          ``ReLU`` non-linearity;\n",
    "\n",
    "        * :math:`C` must be at a multiple of 8.\n",
    "\n",
    "    The MLP :math:`f_{\\\\psi}` is made of custom Linear layers instantiated\n",
    "    from the ``ArmLinear`` class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_arm: int, n_hidden_layers_arm: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_arm: Number of context pixels AND dimension of all hidden\n",
    "                layers :math:`C`.\n",
    "            n_hidden_layers_arm: Number of hidden layers. Set it to 0 for\n",
    "                a linear ARM.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert dim_arm % 8 == 0, (\n",
    "            f\"ARM context size and hidden layer dimension must be \"\n",
    "            f\"a multiple of 8. Found {dim_arm}.\"\n",
    "        )\n",
    "        self.dim_arm = dim_arm\n",
    "        self.hidden_layer_dim = 8\n",
    "\n",
    "        # ======================== Construct the MLP ======================== #\n",
    "        layers_list = nn.ModuleList()\n",
    "        layers_list.append(ArmLinear(dim_arm, self.hidden_layer_dim, residual=False))\n",
    "\n",
    "        # Construct the hidden layer(s)\n",
    "        for i in range(n_hidden_layers_arm):\n",
    "            layers_list.append(\n",
    "                ArmLinear(self.hidden_layer_dim, self.hidden_layer_dim, residual=True)\n",
    "            )\n",
    "            layers_list.append(nn.ReLU())\n",
    "\n",
    "        # Construct the output layer. It always has 2 outputs (mu and scale)\n",
    "        layers_list.append(ArmLinear(self.hidden_layer_dim, 2, residual=False))\n",
    "        self.mlp = nn.Sequential(*layers_list)\n",
    "        # ======================== Construct the MLP ======================== #\n",
    "\n",
    "        self.non_zero_pixel_ctx_index = _get_non_zero_pixel_ctx_index(self.dim_arm)\n",
    "        self.non_zero_pixel_ctx_shifts = {\n",
    "            # row = index // 9, col = index % 9\n",
    "            index: [index // 9 - 4, index % 9 - 4]\n",
    "            for index in self.non_zero_pixel_ctx_index.tolist()\n",
    "        }\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Perform the auto-regressive module (ARM) forward pass. The ARM takes\n",
    "        as input a tensor of shape :math:`[B, C]` i.e. :math:`B` contexts with\n",
    "        :math:`C` context pixels. ARM outputs :math:`[B, 2]` values correspond\n",
    "        to :math:`\\\\mu, b` for each of the :math:`B` input pixels.\n",
    "\n",
    "        .. warning::\n",
    "\n",
    "            Note that the ARM expects input to be flattened i.e. spatial\n",
    "            dimensions :math:`H, W` are collapsed into a single batch-like\n",
    "            dimension :math:`B = HW`, leading to an input of shape\n",
    "            :math:`[B, C]`, gathering the :math:`C` contexts for each of the\n",
    "            :math:`B` pixels to model.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            The ARM MLP does not output directly the scale :math:`b`. Denoting\n",
    "            :math:`s` the raw output of the MLP, the scale is obtained as\n",
    "            follows:\n",
    "\n",
    "            .. math::\n",
    "\n",
    "                b = e^{x - 4}\n",
    "\n",
    "        Args:\n",
    "            x: Concatenation of all input contexts\n",
    "                :math:`\\\\mathbf{c}_i`. Tensor of shape :math:`[B, C]`.\n",
    "\n",
    "        Returns:\n",
    "            Concatenation of all Laplace distributions param :math:`\\\\mu, b`.\n",
    "            Tensor of shape :math:([B]). Also return the *log scale*\n",
    "            :math:`s` as described above. Tensor of shape :math:`(B)`\n",
    "        \"\"\"\n",
    "        raw_proba_param = self.mlp(x)\n",
    "        mu = raw_proba_param[:, 0]\n",
    "        log_scale = raw_proba_param[:, 1]\n",
    "        print(\"ARM mu: \")\n",
    "        for i in range(min(10, mu.size(0))):\n",
    "            print(f\"{mu[i].item():.16f}\")\n",
    "            \n",
    "        # no scale smaller than exp(-4.6) = 1e-2 or bigger than exp(5.01) = 150\n",
    "        scale = torch.exp(torch.clamp(log_scale - 4, min=-4.6, max=5.0))\n",
    "\n",
    "        return mu, scale, log_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "569171ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARM mu: \n",
      "-0.0087932804599404\n",
      "-0.0087932804599404\n",
      "-0.0087932804599404\n",
      "-0.0087932804599404\n",
      "-0.0087932804599404\n",
      "-0.0087932804599404\n",
      "-0.0087932804599404\n",
      "-0.0087932804599404\n",
      "-0.0087932804599404\n",
      "-0.0087932804599404\n",
      "ARM mu: \n",
      "-0.0087932804599404\n",
      "-0.008793280459940434\n",
      "-0.008793280459940434\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    incpecting_index = 1\n",
    "    mu_batch, scale_batch, log_scale_batch = arm_model.forward(latent_context_flat)\n",
    "    mu_singl, scale_singl, log_scale_singl = arm_model.forward(latent_context_flat[incpecting_index:incpecting_index+1])\n",
    "    print(mu_batch[incpecting_index:incpecting_index+1].item())\n",
    "    print(mu_singl.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cool_chic_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
