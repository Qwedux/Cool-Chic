{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94fb52e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"testing\":\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"]=\"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]=\"1\"\n",
    "torch.set_num_threads(1)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.set_printoptions(precision=16, profile=\"full\")\n",
    "np.set_printoptions(precision=16, suppress=False)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "def float_to_full_decimal(x):\n",
    "    \"\"\"Return full decimal precision for numpy or python float.\"\"\"\n",
    "    if isinstance(x, np.floating):\n",
    "        dt = x.dtype\n",
    "        if dt == np.float32:\n",
    "            return format(x, '.9f')\n",
    "        elif dt == np.float64:\n",
    "            return format(x, '.17f')\n",
    "        elif dt == np.float16:\n",
    "            # float16 has ~4 decimal digits\n",
    "            return format(float(x), '.7f')\n",
    "        elif dt == np.float128:\n",
    "            return format(x, '.36f')  # depending on platform\n",
    "        else:\n",
    "            return repr(x)\n",
    "\n",
    "    if isinstance(x, float):  # python float (float64)\n",
    "        return format(x, '.17f')\n",
    "\n",
    "    return repr(x)\n",
    "\n",
    "def float_to_bits(x):\n",
    "    \"\"\"Return IEEE-754 bitstring for numpy or python float.\"\"\"\n",
    "    if isinstance(x, np.floating):\n",
    "        dt = x.dtype\n",
    "        # Use numpy's view machinery\n",
    "        return np.binary_repr(x.view(np.uint64 if dt == np.float64 else np.uint32),\n",
    "                              width=64 if dt == np.float64 else 32)\n",
    "\n",
    "    if isinstance(x, float):\n",
    "        # Python float = IEEE754 float64\n",
    "        return np.binary_repr(np.float64(x).view(np.uint64), width=64)\n",
    "\n",
    "    raise TypeError(f\"Unsupported type: {type(x)}\")\n",
    "\n",
    "def str_full_precision_tensor(a):\n",
    "    \"\"\"\n",
    "    Print every element of a torch.Tensor or numpy.ndarray with:\n",
    "      • full decimal precision\n",
    "      • full binary representation (IEEE754 bits)\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(a, torch.Tensor):\n",
    "        arr = a.detach().cpu().numpy()\n",
    "    elif isinstance(a, np.ndarray):\n",
    "        arr = a\n",
    "    else:\n",
    "        raise TypeError(\"Input must be torch.Tensor or np.ndarray\")\n",
    "\n",
    "    all_indices = list(np.ndindex(arr.shape))\n",
    "    index_strs = [str(idx) for idx in all_indices]\n",
    "    max_idx_len = max(len(s) for s in index_strs)\n",
    "\n",
    "    # print(\"Shape:\", arr.shape)\n",
    "    # print(\"Dtype:\", arr.dtype)\n",
    "    # print()\n",
    "    res_list = []\n",
    "    for idx, x in np.ndenumerate(arr):\n",
    "        idx_str = str(idx).ljust(max_idx_len)\n",
    "        dec = float_to_full_decimal(x)\n",
    "        bits = float_to_bits(x)\n",
    "        res_list.append(f\"{idx_str}: value={dec}, bits={bits}\")\n",
    "    return \"\\n\".join(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2342ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        residual: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = residual\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # -------- Instantiate empty parameters\n",
    "        self.weight = nn.Parameter(torch.empty(out_channels, in_channels), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.empty((out_channels)), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, inspecting_index: int) -> torch.Tensor:\n",
    "        if x.size(0) > 1:\n",
    "            print(f\"IN:  {str_full_precision_tensor(x[inspecting_index:inspecting_index+1])}\")\n",
    "        else:\n",
    "            print(f\"IN:  {str_full_precision_tensor(x)}\")\n",
    "\n",
    "        lin = x @ self.weight.t() + self.bias\n",
    "\n",
    "        if self.residual:\n",
    "            res = lin + x\n",
    "        else:\n",
    "            res = lin\n",
    "        if x.size(0) > 1:\n",
    "            print(f\"OUT: {str_full_precision_tensor(res[inspecting_index:inspecting_index+1])}\")\n",
    "        else:\n",
    "            print(f\"OUT: {str_full_precision_tensor(res)}\")\n",
    "        return res\n",
    "\n",
    "\n",
    "class SequentialLike(nn.Module):\n",
    "    def __init__(self, *layers: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        for i, layer in enumerate(layers):\n",
    "            self.add_module(str(i), layer)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, inspecting_index: int) -> torch.Tensor:\n",
    "        for i, layer in enumerate(self.children()):  # iterates in registration order\n",
    "            if i <= 1:\n",
    "                if isinstance(layer, ArmLinear):\n",
    "                    x = layer(x, inspecting_index)\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Arm(nn.Module):\n",
    "    def __init__(self, dim_arm: int, n_hidden_layers_arm: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_arm = dim_arm\n",
    "        self.hidden_layer_dim = 8\n",
    "\n",
    "        # ======================== Construct the MLP ======================== #\n",
    "        layers_list = nn.ModuleList()\n",
    "        layers_list.append(ArmLinear(dim_arm, self.hidden_layer_dim, residual=False))\n",
    "\n",
    "        # Construct the hidden layer(s)\n",
    "        for _ in range(n_hidden_layers_arm):\n",
    "            layers_list.append(\n",
    "                ArmLinear(self.hidden_layer_dim, self.hidden_layer_dim, residual=True)\n",
    "            )\n",
    "            layers_list.append(nn.ReLU())\n",
    "\n",
    "        # Construct the output layer. It always has 2 outputs (mu and scale)\n",
    "        layers_list.append(ArmLinear(self.hidden_layer_dim, 2, residual=False))\n",
    "        self.mlp = SequentialLike(*layers_list)\n",
    "        # ======================== Construct the MLP ======================== #\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, inspecting_index: int\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - tensor of shape (B, C), where C = dim_arm\n",
    "        \"\"\"\n",
    "        raw_proba_param = self.mlp(x, inspecting_index)\n",
    "        mu = raw_proba_param[:, 0]\n",
    "        log_scale = raw_proba_param[:, 1]\n",
    "\n",
    "        # no scale smaller than exp(-4.6) = 1e-2 or bigger than exp(5.01) = 150\n",
    "        scale = torch.exp(torch.clamp(log_scale - 4, min=-4.6, max=5.0))\n",
    "\n",
    "        return mu, scale, log_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa2342ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IN:  (0, 0) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 1) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 2) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 3) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 4) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 5) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 6) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 7) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 8) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 9) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 10): value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 11): value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 12): value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 13): value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 14): value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 15): value=3.000000000, bits=01000000010000000000000000000000\n",
      "OUT: (0, 0): value=5.469199181, bits=01000000101011110000001110101110\n",
      "(0, 1): value=6.786665916, bits=01000000110110010010110001011110\n",
      "(0, 2): value=-0.776918769, bits=10111111010001101110010000100110\n",
      "(0, 3): value=0.523305178, bits=00111111000001011111011101010100\n",
      "(0, 4): value=-1.626949430, bits=10111111110100000011111111100001\n",
      "(0, 5): value=0.258832008, bits=00111110100001001000010110100001\n",
      "(0, 6): value=-0.213482454, bits=10111110010110101001101100100101\n",
      "(0, 7): value=-1.559148669, bits=10111111110001111001001000101111\n",
      "IN:  (0, 0): value=5.469199181, bits=01000000101011110000001110101110\n",
      "(0, 1): value=6.786665916, bits=01000000110110010010110001011110\n",
      "(0, 2): value=-0.776918769, bits=10111111010001101110010000100110\n",
      "(0, 3): value=0.523305178, bits=00111111000001011111011101010100\n",
      "(0, 4): value=-1.626949430, bits=10111111110100000011111111100001\n",
      "(0, 5): value=0.258832008, bits=00111110100001001000010110100001\n",
      "(0, 6): value=-0.213482454, bits=10111110010110101001101100100101\n",
      "(0, 7): value=-1.559148669, bits=10111111110001111001001000101111\n",
      "OUT: (0, 0): value=12.551399231, bits=01000001010010001101001010001000\n",
      "(0, 1): value=11.537749290, bits=01000001001110001001101010011111\n",
      "(0, 2): value=15.117259979, bits=01000001011100011110000001001100\n",
      "(0, 3): value=-6.933016777, bits=11000000110111011101101101000110\n",
      "(0, 4): value=0.905846953, bits=00111111011001111110010110010110\n",
      "(0, 5): value=-2.828150749, bits=11000000001101010000000001101100\n",
      "(0, 6): value=1.453665853, bits=00111111101110100001000110111001\n",
      "(0, 7): value=-5.022366524, bits=11000000101000001011011100111010\n",
      "================================================================================\n",
      "IN:  (0, 0) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 1) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 2) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 3) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 4) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 5) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 6) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 7) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 8) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 9) : value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 10): value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 11): value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 12): value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 13): value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 14): value=0.000000000, bits=00000000000000000000000000000000\n",
      "(0, 15): value=3.000000000, bits=01000000010000000000000000000000\n",
      "OUT: (0, 0): value=5.469199181, bits=01000000101011110000001110101110\n",
      "(0, 1): value=6.786665916, bits=01000000110110010010110001011110\n",
      "(0, 2): value=-0.776918769, bits=10111111010001101110010000100110\n",
      "(0, 3): value=0.523305178, bits=00111111000001011111011101010100\n",
      "(0, 4): value=-1.626949430, bits=10111111110100000011111111100001\n",
      "(0, 5): value=0.258832008, bits=00111110100001001000010110100001\n",
      "(0, 6): value=-0.213482454, bits=10111110010110101001101100100101\n",
      "(0, 7): value=-1.559148669, bits=10111111110001111001001000101111\n",
      "IN:  (0, 0): value=5.469199181, bits=01000000101011110000001110101110\n",
      "(0, 1): value=6.786665916, bits=01000000110110010010110001011110\n",
      "(0, 2): value=-0.776918769, bits=10111111010001101110010000100110\n",
      "(0, 3): value=0.523305178, bits=00111111000001011111011101010100\n",
      "(0, 4): value=-1.626949430, bits=10111111110100000011111111100001\n",
      "(0, 5): value=0.258832008, bits=00111110100001001000010110100001\n",
      "(0, 6): value=-0.213482454, bits=10111110010110101001101100100101\n",
      "(0, 7): value=-1.559148669, bits=10111111110001111001001000101111\n",
      "OUT: (0, 0): value=12.551398277, bits=01000001010010001101001010000111\n",
      "(0, 1): value=11.537749290, bits=01000001001110001001101010011111\n",
      "(0, 2): value=15.117259979, bits=01000001011100011110000001001100\n",
      "(0, 3): value=-6.933017731, bits=11000000110111011101101101001000\n",
      "(0, 4): value=0.905847192, bits=00111111011001111110010110011010\n",
      "(0, 5): value=-2.828150988, bits=11000000001101010000000001101101\n",
      "(0, 6): value=1.453665853, bits=00111111101110100001000110111001\n",
      "(0, 7): value=-5.022366524, bits=11000000101000001011011100111010\n",
      "================================================================================\n",
      "(0,): value=12.551399231, bits=01000001010010001101001010001000\n",
      "(0,): value=12.551398277, bits=01000001010010001101001010000111\n"
     ]
    }
   ],
   "source": [
    "arm_model = Arm(dim_arm=16, n_hidden_layers_arm=2).to(DEVICE)\n",
    "# load pretrained weights\n",
    "arm_model.load_state_dict(\n",
    "    torch.load(\"../logs/full_runs/trained_models/coolchic_arm.pth\", map_location=DEVICE)\n",
    ")\n",
    "arm_model.eval()\n",
    "\n",
    "latents_dict = torch.load(\n",
    "    \"../logs/full_runs/trained_models/coolchic_latents_snapshot.pt\", map_location=DEVICE\n",
    ")\n",
    "flat_latent = latents_dict[\"flat_latent\"].to(DEVICE)\n",
    "latent_context_flat = latents_dict[\"latent_context_flat\"].to(DEVICE)\n",
    "# latents = torch.zeros((1,1,10,10), dtype=torch.float32).to(DEVICE)\n",
    "# latent_context_flat = torch.concat([arm_model.get_neighbor_context(latents[0,0].tolist(), i, j) for j in range(latents.shape[3]) for i in range(latents.shape[2])])[:10]\n",
    "\n",
    "with torch.no_grad():\n",
    "    incpecting_index = 1\n",
    "    print(\"=\"*80)\n",
    "    mu_batch, scale_batch, log_scale_batch = arm_model.forward(latent_context_flat, incpecting_index)\n",
    "    print(\"=\"*80)\n",
    "    mu_singl, scale_singl, log_scale_singl = arm_model.forward(\n",
    "        latent_context_flat[incpecting_index : incpecting_index + 1], incpecting_index\n",
    "    )\n",
    "    print(\"=\"*80)\n",
    "    print(str_full_precision_tensor(mu_batch[incpecting_index : incpecting_index + 1]))\n",
    "    print(str_full_precision_tensor(mu_singl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
