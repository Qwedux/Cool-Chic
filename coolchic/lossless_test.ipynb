{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30588d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim01.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim02.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim03.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim04.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim05.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim06.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim07.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim08.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim09.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim10.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim11.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim12.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim13.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim14.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim15.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim16.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim17.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim18.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim19.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim20.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim21.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim22.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim23.png', '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim24.png']\n",
      "{'input': '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim01.png', 'output': '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/test-workdir//output', 'workdir': '/home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/test-workdir/', 'lmbda': 0.001, 'job_duration_min': -1, 'print_detailed_archi': False, 'print_detailed_struct': False, 'start_lr': 0.01, 'n_itr': 1, 'n_itr_pretrain_motion': 1, 'n_train_loops': 1, 'preset': 'debug', 'layers_synthesis_residue': '16-1-linear-relu,X-1-linear-none,X-3-residual-relu,X-3-residual-none', 'arm_residue': '8,2', 'n_ft_per_res_residue': '1,1,1,1,1,1,1', 'ups_k_size_residue': 8, 'ups_preconcat_k_size_residue': 7}\n",
      "----------\n",
      "\n",
      "\n",
      "*----------------------------------------------------------------------------------------------------------*\n",
      "|                                                                                                          |\n",
      "|                                                                                                          |\n",
      "|       ,gggg,                                                                                             |\n",
      "|     ,88\"\"\"Y8b,                           ,dPYb,                             ,dPYb,                       |\n",
      "|    d8\"     `Y8                           IP'`Yb                             IP'`Yb                       |\n",
      "|   d8'   8b  d8                           I8  8I                             I8  8I      gg               |\n",
      "|  ,8I    \"Y88P'                           I8  8'                             I8  8'      \"\"               |\n",
      "|  I8'             ,ggggg,      ,ggggg,    I8 dP      aaaaaaaa        ,gggg,  I8 dPgg,    gg     ,gggg,    |\n",
      "|  d8             dP\"  \"Y8ggg  dP\"  \"Y8ggg I8dP       \"\"\"\"\"\"\"\"       dP\"  \"Yb I8dP\" \"8I   88    dP\"  \"Yb   |\n",
      "|  Y8,           i8'    ,8I   i8'    ,8I   I8P                      i8'       I8P    I8   88   i8'         |\n",
      "|  `Yba,,_____, ,d8,   ,d8'  ,d8,   ,d8'  ,d8b,_                   ,d8,_    _,d8     I8,_,88,_,d8,_    _   |\n",
      "|    `\"Y8888888 P\"Y8888P\"    P\"Y8888P\"    8P'\"Y88                  P\"\"Y8888PP88P     `Y88P\"\"Y8P\"\"Y8888PP   |\n",
      "|                                                                                                          |\n",
      "|                                                                                                          |\n",
      "| version 4.1.0, July 2025                                                              © 2023-2025 Orange |\n",
      "*----------------------------------------------------------------------------------------------------------*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import torch\n",
    "from lossless.component.coolchic import CoolChicEncoderParameter\n",
    "from lossless.component.frame import load_frame_encoder\n",
    "from lossless.component.types import NAME_COOLCHIC_ENC\n",
    "from lossless.component.image import (\n",
    "    FrameEncoderManager,\n",
    "    encode_one_frame,\n",
    ")\n",
    "from enc.utils.codingstructure import CodingStructure, Frame\n",
    "from typing import Any, Dict, List\n",
    "from lossless.component.coolchic import CoolChicEncoder\n",
    "\n",
    "DATASET_PATH = f\"{os.getcwd()}/../datasets/kodak\"\n",
    "IMAGE_PATHS = sorted(\n",
    "    glob.glob(f\"{DATASET_PATH}/*.png\"),\n",
    "    key=lambda x: int(os.path.basename(x).split(\".\")[0][len(\"kodim\") :]),\n",
    ")\n",
    "TEST_WORKDIR = f\"{os.getcwd()}/test-workdir/\"\n",
    "PATH_COOL_CHIC_CFG = f\"{os.getcwd()}/../cfg/\"\n",
    "print(IMAGE_PATHS)\n",
    "\n",
    "args = {\n",
    "    # not in config files\n",
    "    \"input\": IMAGE_PATHS[0],\n",
    "    \"output\": TEST_WORKDIR + \"/output\",\n",
    "    \"workdir\": TEST_WORKDIR,\n",
    "    \"lmbda\": 1e-3,\n",
    "    \"job_duration_min\": -1,\n",
    "    \"print_detailed_archi\": False,\n",
    "    \"print_detailed_struct\": False,\n",
    "    # config file paths\n",
    "    # encoder side\n",
    "    \"start_lr\": 1e-2,\n",
    "    \"n_itr\": 1,\n",
    "    \"n_itr_pretrain_motion\": 1,\n",
    "    \"n_train_loops\": 1,\n",
    "    \"preset\": \"debug\",\n",
    "    # decoder side\n",
    "    \"layers_synthesis_residue\": \"16-1-linear-relu,X-1-linear-none,X-3-residual-relu,X-3-residual-none\",\n",
    "    \"arm_residue\": \"8,2\",\n",
    "    \"n_ft_per_res_residue\": \"1,1,1,1,1,1,1\",\n",
    "    \"ups_k_size_residue\": 8,\n",
    "    \"ups_preconcat_k_size_residue\": 7,\n",
    "}\n",
    "\n",
    "print(args)\n",
    "print(\"----------\")\n",
    "# os.chdir(args[\"workdir\"])\n",
    "\n",
    "start_print = (\n",
    "    \"\\n\\n\"\n",
    "    \"*----------------------------------------------------------------------------------------------------------*\\n\"\n",
    "    \"|                                                                                                          |\\n\"\n",
    "    \"|                                                                                                          |\\n\"\n",
    "    \"|       ,gggg,                                                                                             |\\n\"\n",
    "    '|     ,88\"\"\"Y8b,                           ,dPYb,                             ,dPYb,                       |\\n'\n",
    "    \"|    d8\\\"     `Y8                           IP'`Yb                             IP'`Yb                       |\\n\"\n",
    "    \"|   d8'   8b  d8                           I8  8I                             I8  8I      gg               |\\n\"\n",
    "    \"|  ,8I    \\\"Y88P'                           I8  8'                             I8  8'      \\\"\\\"               |\\n\"\n",
    "    \"|  I8'             ,ggggg,      ,ggggg,    I8 dP      aaaaaaaa        ,gggg,  I8 dPgg,    gg     ,gggg,    |\\n\"\n",
    "    '|  d8             dP\"  \"Y8ggg  dP\"  \"Y8ggg I8dP       \"\"\"\"\"\"\"\"       dP\"  \"Yb I8dP\" \"8I   88    dP\"  \"Yb   |\\n'\n",
    "    \"|  Y8,           i8'    ,8I   i8'    ,8I   I8P                      i8'       I8P    I8   88   i8'         |\\n\"\n",
    "    \"|  `Yba,,_____, ,d8,   ,d8'  ,d8,   ,d8'  ,d8b,_                   ,d8,_    _,d8     I8,_,88,_,d8,_    _   |\\n\"\n",
    "    '|    `\"Y8888888 P\"Y8888P\"    P\"Y8888P\"    8P\\'\"Y88                  P\"\"Y8888PP88P     `Y88P\"\"Y8P\"\"Y8888PP   |\\n'\n",
    "    \"|                                                                                                          |\\n\"\n",
    "    \"|                                                                                                          |\\n\"\n",
    "    \"| version 4.1.0, July 2025                                                              © 2023-2025 Orange |\\n\"\n",
    "    \"*----------------------------------------------------------------------------------------------------------*\\n\"\n",
    ")\n",
    "print(start_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab56161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTIL CODE\n",
    "\n",
    "def pretty_str_dict(d: dict[str, Any]) -> str:\n",
    "    if not d:\n",
    "        return \"\"\n",
    "    \n",
    "    # Find length of the longest key\n",
    "    max_key_len = max(len(k) for k in d.keys())\n",
    "    \n",
    "    lines = []\n",
    "    for key, value in d.items():\n",
    "        # Pad key so values align, ensure at least one space after colon\n",
    "        lines.append(f\"{key}:{' ' * (max_key_len - len(key) + 1)}{value}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c1f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COOL CHICK PARAMETER PARSER CODE\n",
    "def parse_synthesis_layers(layers_synthesis: str) -> List[str]:\n",
    "    \"\"\"The layers of the synthesis are presented in as a coma-separated string.\n",
    "    This simply splits up the different substrings and return them.\n",
    "\n",
    "    Args:\n",
    "        layers_synthesis (str): Command line argument for the synthesis.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of string where the i-th element described the i-th\n",
    "            synthesis layer\n",
    "    \"\"\"\n",
    "    parsed_layer_synth = [x for x in layers_synthesis.split(\",\") if x != \"\"]\n",
    "\n",
    "    assert parsed_layer_synth, (\n",
    "        \"Synthesis should have at least one layer, found nothing. \\n\"\n",
    "        f\"--layers_synthesis={layers_synthesis} does not work!\\n\"\n",
    "        \"Try something like 32-1-linear-relu,X-1-linear-none,\"\n",
    "        \"X-3-residual-relu,X-3-residual-none\"\n",
    "    )\n",
    "\n",
    "    return parsed_layer_synth\n",
    "\n",
    "\n",
    "def parse_n_ft_per_res(n_ft_per_res: str) -> list[int]:\n",
    "    \"\"\"The number of feature per resolution is a coma-separated string.\n",
    "    This simply splits up the different substrings and return them.\n",
    "\n",
    "    Args:\n",
    "        n_ft_per_res (str): Something like \"1,1,1,1,1,1,1\" for 7 latent grids\n",
    "        with different resolution and 1 feature each.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: The i-th element is the number of features for the i-th\n",
    "        latent, i.e. the latent of a resolution (H / 2^i, W / 2^i).\n",
    "    \"\"\"\n",
    "\n",
    "    n_ft_per_res_int = [int(x) for x in n_ft_per_res.split(\",\") if x != \"\"]\n",
    "    # assert set(n_ft_per_res) == {\n",
    "    #     1\n",
    "    # }, f\"--n_ft_per_res should only contains 1. Found {n_ft_per_res}\"\n",
    "    return n_ft_per_res_int\n",
    "\n",
    "\n",
    "def parse_arm_archi(arm: str) -> Dict[str, int]:\n",
    "    \"\"\"The arm is described as <dim_arm>,<n_hidden_layers_arm>.\n",
    "    Split up this string to return the value as a dict.\n",
    "\n",
    "    Args:\n",
    "        arm (str): Command line argument for the ARM.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: The ARM architecture\n",
    "    \"\"\"\n",
    "    assert len(arm.split(\",\")) == 2, (\n",
    "        f\"--arm format should be X,Y.\" f\" Found {arm}\"\n",
    "    )\n",
    "\n",
    "    dim_arm, n_hidden_layers_arm = [int(x) for x in arm.split(\",\")]\n",
    "    arm_param = {\"dim_arm\": dim_arm, \"n_hidden_layers_arm\": n_hidden_layers_arm}\n",
    "    return arm_param\n",
    "\n",
    "\n",
    "def get_coolchic_param_from_args(\n",
    "    args: dict,\n",
    "    coolchic_enc_name: str,\n",
    ") -> Dict[str, Any]:\n",
    "    layers_synthesis = parse_synthesis_layers(\n",
    "        args[f\"layers_synthesis_{coolchic_enc_name}\"]\n",
    "    )\n",
    "    n_ft_per_res = parse_n_ft_per_res(args[f\"n_ft_per_res_{coolchic_enc_name}\"])\n",
    "\n",
    "    coolchic_param = {\n",
    "        \"layers_synthesis\": layers_synthesis,\n",
    "        \"n_ft_per_res\": n_ft_per_res,\n",
    "        \"ups_k_size\": args[f\"ups_k_size_{coolchic_enc_name}\"],\n",
    "        \"ups_preconcat_k_size\": args[\n",
    "            f\"ups_preconcat_k_size_{coolchic_enc_name}\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Add ARM parameters\n",
    "    coolchic_param.update(parse_arm_archi(args[f\"arm_{coolchic_enc_name}\"]))\n",
    "\n",
    "    return coolchic_param\n",
    "\n",
    "def change_n_out_synth(layers_synth: List[str], n_out: int) -> List[str]:\n",
    "        \"\"\"Change the number of output features in the list of strings\n",
    "        describing the synthesis architecture. It replaces \"X\" with n_out. E.g.\n",
    "\n",
    "        From [8-1-linear-relu,X-1-linear-none,X-3-residual-none]\n",
    "        To   [8-1-linear-relu,2-1-linear-none,2-3-residual-none]\n",
    "\n",
    "        If n_out = 2\n",
    "\n",
    "        Args:\n",
    "            layers_synth (List[str]): List of strings describing the different\n",
    "                synthesis layers\n",
    "            n_out (int): Number of desired output.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of strings with the proper number of output features.\n",
    "        \"\"\"\n",
    "        return [lay.replace(\"X\", str(n_out)) for lay in layers_synth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14db32c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing /home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/test-workdir/...\n"
     ]
    }
   ],
   "source": [
    "# remove the content of the workdir if it exists\n",
    "if os.path.exists(args[\"workdir\"]):\n",
    "    print(f\"Removing {args['workdir']}...\")\n",
    "    for file in os.listdir(args[\"workdir\"]):\n",
    "        file_path = os.path.join(args[\"workdir\"], file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a790acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoolChicEncoder(\n",
       "  (latent_grids): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 1x1x768x512]\n",
       "      (1): Parameter containing: [torch.float32 of size 1x1x384x256]\n",
       "      (2): Parameter containing: [torch.float32 of size 1x1x192x128]\n",
       "      (3): Parameter containing: [torch.float32 of size 1x1x96x64]\n",
       "      (4): Parameter containing: [torch.float32 of size 1x1x48x32]\n",
       "      (5): Parameter containing: [torch.float32 of size 1x1x24x16]\n",
       "      (6): Parameter containing: [torch.float32 of size 1x1x12x8]\n",
       "  )\n",
       "  (synthesis): Synthesis(\n",
       "    (synth_branches): ModuleList()\n",
       "    (layers): Sequential(\n",
       "      (0): SynthesisConv2d()\n",
       "      (1): ReLU()\n",
       "      (2): SynthesisConv2d()\n",
       "      (3): Identity()\n",
       "      (4): SynthesisConv2d()\n",
       "      (5): ReLU()\n",
       "      (6): SynthesisConv2d()\n",
       "      (7): Identity()\n",
       "    )\n",
       "  )\n",
       "  (upsampling): Upsampling(\n",
       "    (conv_transpose2ds): ModuleList(\n",
       "      (0-5): 6 x ParametrizedUpsamplingSeparableSymmetricConvTranspose2d(\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _Parameterization_Symmetric_1d()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv2ds): ModuleList(\n",
       "      (0-5): 6 x ParametrizedUpsamplingSeparableSymmetricConv2d(\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _Parameterization_Symmetric_1d()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (arm): Arm(\n",
       "    (mlp): Sequential(\n",
       "      (0): ArmLinear()\n",
       "      (1): ReLU()\n",
       "      (2): ArmLinear()\n",
       "      (3): ReLU()\n",
       "      (4): ArmLinear()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "encoder_param = CoolChicEncoderParameter(\n",
    "    **get_coolchic_param_from_args(args, \"residue\")\n",
    ")\n",
    "encoder_param.set_image_size((768, 512))\n",
    "encoder_param.layers_synthesis = change_n_out_synth(\n",
    "    encoder_param.layers_synthesis, 6\n",
    ")\n",
    "\n",
    "coolchic = CoolChicEncoder(param=encoder_param)\n",
    "coolchic.eval()\n",
    "\n",
    "# print(coolchic.pretty_string(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "789c05f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random prior: dict_keys(['raw_out', 'rate', 'additional_data'])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Forward pass with no quantization noise\n",
    "    # This is a random prior, i.e. the output is not conditioned on any input\n",
    "    # image.\n",
    "    random_prior = coolchic.forward(\n",
    "            quantizer_noise_type=\"none\",\n",
    "            quantizer_type=\"hardround\",\n",
    "            AC_MAX_VAL=-1,\n",
    "            flag_additional_outputs=False,\n",
    "        )\n",
    "print(f\"Random prior: {random_prior.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cd923a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 768, 512])\n",
      "torch.Size([524256])\n",
      "tensor(0.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(random_prior[\"raw_out\"].size())\n",
    "print(random_prior[\"rate\"].size())\n",
    "# print(random_prior[\"additional_data\"])\n",
    "print(torch.min(random_prior[\"raw_out\"]), torch.max(random_prior[\"raw_out\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0610560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 PERFORMING COOL CHIC FORWARD PASS (ENCODING + DECODING)\n",
      "============================================================\n",
      "✅ Decoding completed!\n",
      "   Input latent grids: 7 hierarchical levels\n",
      "   Decoded image shape: torch.Size([1, 3, 768, 512])\n",
      "   Pixel value range: [0.000, 0.000]\n",
      "   Total rate: 0.0 bits\n",
      "   Clamped pixel range: [0.000, 0.000]\n",
      "\n",
      "🎯 KEY POINTS:\n",
      "   • coolchic() performs complete encoding+decoding pipeline\n",
      "   • output['raw_out'] contains the decoded RGB image\n",
      "   • output['rate'] contains compression rate information\n",
      "   • For lossy: synthesis outputs RGB pixels directly\n",
      "   • For lossless: synthesis should output distribution parameters\n"
     ]
    }
   ],
   "source": [
    "# DECODING WITH COOL CHIC ENCODER\n",
    "# The CoolChicEncoder.forward() method performs the complete encoding process\n",
    "# and returns a CoolChicEncoderOutput containing the decoded image\n",
    "\n",
    "print(\"🔄 PERFORMING COOL CHIC FORWARD PASS (ENCODING + DECODING)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Put the encoder in evaluation mode for inference\n",
    "coolchic.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Forward pass through the Cool Chic encoder\n",
    "    # This performs the complete encoding and decoding pipeline:\n",
    "    # 1. Quantize latent variables\n",
    "    # 2. ARM predicts latent distributions for rate calculation  \n",
    "    # 3. Upsampling takes latents to full resolution\n",
    "    # 4. Synthesis converts features to RGB pixels\n",
    "    \n",
    "    output = coolchic()\n",
    "    \n",
    "    # Extract the decoded image from the output\n",
    "    decoded_image = output[\"raw_out\"]  # [1, 3, H, W] tensor\n",
    "    rate_bits = output[\"rate\"]         # Rate in bits for compression\n",
    "    \n",
    "    print(f\"✅ Decoding completed!\")\n",
    "    print(f\"   Input latent grids: {len(coolchic.latent_grids)} hierarchical levels\")\n",
    "    print(f\"   Decoded image shape: {decoded_image.shape}\")\n",
    "    print(f\"   Pixel value range: [{decoded_image.min():.3f}, {decoded_image.max():.3f}]\")\n",
    "    print(f\"   Total rate: {rate_bits.sum():.1f} bits\")\n",
    "    \n",
    "    # The decoded image is now ready to use!\n",
    "    # For visualization, clamp to valid range [0, 1]\n",
    "    decoded_pixels = torch.clamp(decoded_image, 0.0, 1.0)\n",
    "    print(f\"   Clamped pixel range: [{decoded_pixels.min():.3f}, {decoded_pixels.max():.3f}]\")\n",
    "\n",
    "print(\"\\n🎯 KEY POINTS:\")\n",
    "print(\"   • coolchic() performs complete encoding+decoding pipeline\")\n",
    "print(\"   • output['raw_out'] contains the decoded RGB image\")\n",
    "print(\"   • output['rate'] contains compression rate information\")\n",
    "print(\"   • For lossy: synthesis outputs RGB pixels directly\")\n",
    "print(\"   • For lossless: synthesis should output distribution parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e35e5298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 MANUAL DECODING STEP-BY-STEP\n",
      "==================================================\n",
      "Step 1: LATENT VARIABLES (learned representation)\n",
      "   Grid 0: torch.Size([1, 1, 768, 512]) - mean=0.0000, std=0.0000\n",
      "   Grid 1: torch.Size([1, 1, 384, 256]) - mean=0.0000, std=0.0000\n",
      "   Grid 2: torch.Size([1, 1, 192, 128]) - mean=0.0000, std=0.0000\n",
      "   Grid 3: torch.Size([1, 1, 96, 64]) - mean=0.0000, std=0.0000\n",
      "   Grid 4: torch.Size([1, 1, 48, 32]) - mean=0.0000, std=0.0000\n",
      "   Grid 5: torch.Size([1, 1, 24, 16]) - mean=0.0000, std=0.0000\n",
      "   Grid 6: torch.Size([1, 1, 12, 8]) - mean=0.0000, std=0.0000\n",
      "\n",
      "Step 2: AUTO-REGRESSIVE MODULE (ARM)\n",
      "   → Predicts probability distributions (μ, σ) for each latent\n",
      "   → ARM processes spatial context to predict latent statistics\n",
      "   ARM architecture: 16-D hidden layers\n",
      "\n",
      "Step 3: UPSAMPLING NETWORK\n",
      "   → Takes hierarchical latents and upsamples to full resolution\n",
      "   Input: 7 grids at different resolutions\n",
      "   Output: torch.Size([1, 7, 768, 512]) (full resolution features)\n",
      "\n",
      "Step 4: SYNTHESIS NETWORK ⭐ KEY FOR LOSSLESS\n",
      "   → Current: Outputs RGB pixel values directly\n",
      "   → For lossless: Should output logistic distribution parameters\n",
      "   Current output shape: torch.Size([1, 3, 768, 512])\n",
      "   Range: [0.000, 0.000]\n",
      "\n",
      "💡 FOR LOSSLESS COMPRESSION:\n",
      "   • Steps 1-3 stay the same\n",
      "   • Step 4 (Synthesis) should output distribution parameters instead of pixels\n",
      "   • Then use entropy coding to get exact pixel values\n",
      "\n",
      "🔄 HOW TO USE FOR DECODING:\n",
      "   1. Call coolchic() to get decoded image: output = coolchic()\n",
      "   2. Extract image: decoded_img = output['raw_out']\n",
      "   3. Clamp to valid range: final_img = torch.clamp(decoded_img, 0, 1)\n",
      "   4. Convert to numpy if needed: img_np = final_img.squeeze().permute(1,2,0).numpy()\n"
     ]
    }
   ],
   "source": [
    "# MANUAL STEP-BY-STEP DECODING BREAKDOWN\n",
    "print(\"\\n🔧 MANUAL DECODING STEP-BY-STEP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"Step 1: LATENT VARIABLES (learned representation)\")\n",
    "    latent_grids = [grid.data for grid in coolchic.latent_grids]\n",
    "    for i, grid in enumerate(latent_grids):\n",
    "        print(f\"   Grid {i}: {grid.shape} - mean={grid.mean():.4f}, std={grid.std():.4f}\")\n",
    "    \n",
    "    print(\"\\nStep 2: AUTO-REGRESSIVE MODULE (ARM)\")\n",
    "    print(\"   → Predicts probability distributions (μ, σ) for each latent\")\n",
    "    print(\"   → ARM processes spatial context to predict latent statistics\")\n",
    "    print(f\"   ARM architecture: {coolchic.param.dim_arm}-D hidden layers\")\n",
    "    \n",
    "    print(\"\\nStep 3: UPSAMPLING NETWORK\")\n",
    "    print(\"   → Takes hierarchical latents and upsamples to full resolution\")\n",
    "    upsampled_features = coolchic.upsampling(latent_grids)\n",
    "    print(f\"   Input: {len(latent_grids)} grids at different resolutions\")\n",
    "    print(f\"   Output: {upsampled_features.shape} (full resolution features)\")\n",
    "    \n",
    "    print(\"\\nStep 4: SYNTHESIS NETWORK ⭐ KEY FOR LOSSLESS\")\n",
    "    print(\"   → Current: Outputs RGB pixel values directly\")\n",
    "    print(\"   → For lossless: Should output logistic distribution parameters\")\n",
    "    synthesized_output = coolchic.synthesis(upsampled_features)\n",
    "    print(f\"   Current output shape: {synthesized_output.shape}\")\n",
    "    print(f\"   Range: [{synthesized_output.min():.3f}, {synthesized_output.max():.3f}]\")\n",
    "    \n",
    "    # Final resize to image dimensions if needed\n",
    "    if synthesized_output.shape[-2:] != coolchic.param.img_size:\n",
    "        final_output = torch.nn.functional.interpolate(\n",
    "            synthesized_output, \n",
    "            size=coolchic.param.img_size, \n",
    "            mode=\"nearest\"\n",
    "        )\n",
    "        print(f\"   Resized to: {final_output.shape}\")\n",
    "    else:\n",
    "        final_output = synthesized_output\n",
    "    \n",
    "print(\"\\n💡 FOR LOSSLESS COMPRESSION:\")\n",
    "print(\"   • Steps 1-3 stay the same\")\n",
    "print(\"   • Step 4 (Synthesis) should output distribution parameters instead of pixels\")\n",
    "print(\"   • Then use entropy coding to get exact pixel values\")\n",
    "\n",
    "print(\"\\n🔄 HOW TO USE FOR DECODING:\")\n",
    "print(\"   1. Call coolchic() to get decoded image: output = coolchic()\")\n",
    "print(\"   2. Extract image: decoded_img = output['raw_out']\")\n",
    "print(\"   3. Clamp to valid range: final_img = torch.clamp(decoded_img, 0, 1)\")\n",
    "print(\"   4. Convert to numpy if needed: img_np = final_img.squeeze().permute(1,2,0).numpy()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c1d4c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ PRACTICAL DECODING EXAMPLE\n",
      "========================================\n",
      "Loading image: /home/jakub/ETH/2025_2026_fall/thesis/Cool-Chic/coolchic/../datasets/kodak/kodim01.png\n",
      "Original image shape: torch.Size([1, 3, 512, 768])\n",
      "Pixel range: [0.000, 1.000]\n",
      "\n",
      "🔧 Initializing latent grids with random values...\n",
      "   Grid 0: std=0.1001\n",
      "   Grid 1: std=0.1000\n",
      "   Grid 2: std=0.0998\n",
      "   Grid 3: std=0.1003\n",
      "   Grid 4: std=0.0978\n",
      "   Grid 5: std=0.1101\n",
      "   Grid 6: std=0.1032\n",
      "\n",
      "🚀 PERFORMING DECODING...\n",
      "✅ Decoding successful!\n",
      "   Decoded shape: torch.Size([1, 3, 768, 512])\n",
      "   Pixel range: [0.000, 0.000]\n",
      "   Rate: 6373600.5 bits\n",
      "\n",
      "📊 SUMMARY:\n",
      "   • Input: torch.Size([1, 3, 512, 768]) original image\n",
      "   • Output: torch.Size([1, 3, 768, 512]) decoded image\n",
      "   • The synthesis network converted 7 features → 3 RGB channels\n",
      "   • This is the core of the Cool-Chic decoder!\n"
     ]
    }
   ],
   "source": [
    "# PRACTICAL EXAMPLE: LOAD IMAGE AND DECODE\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "print(\"🖼️ PRACTICAL DECODING EXAMPLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check if we have access to an image\n",
    "if IMAGE_PATHS:\n",
    "    print(f\"Loading image: {IMAGE_PATHS[0]}\")\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    img = Image.open(IMAGE_PATHS[0]).convert('RGB')\n",
    "    img_array = np.array(img).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Convert to PyTorch tensor [1, 3, H, W]\n",
    "    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "    print(f\"Original image shape: {img_tensor.shape}\")\n",
    "    print(f\"Pixel range: [{img_tensor.min():.3f}, {img_tensor.max():.3f}]\")\n",
    "    \n",
    "    # Initialize latent grids with some non-zero values for demo\n",
    "    # (In practice, these would be optimized during training)\n",
    "    print(\"\\n🔧 Initializing latent grids with random values...\")\n",
    "    for i, grid in enumerate(coolchic.latent_grids):\n",
    "        grid.data.normal_(0, 0.1)  # Small random initialization\n",
    "        print(f\"   Grid {i}: std={grid.data.std():.4f}\")\n",
    "    \n",
    "    # Now perform decoding\n",
    "    print(\"\\n🚀 PERFORMING DECODING...\")\n",
    "    coolchic.eval()\n",
    "    with torch.no_grad():\n",
    "        output = coolchic()\n",
    "        decoded_image = output[\"raw_out\"]\n",
    "        rate_bits = output[\"rate\"]\n",
    "        \n",
    "        print(f\"✅ Decoding successful!\")\n",
    "        print(f\"   Decoded shape: {decoded_image.shape}\")\n",
    "        print(f\"   Pixel range: [{decoded_image.min():.3f}, {decoded_image.max():.3f}]\")\n",
    "        print(f\"   Rate: {rate_bits.sum():.1f} bits\")\n",
    "        \n",
    "        # Clamp to valid range\n",
    "        decoded_clamped = torch.clamp(decoded_image, 0.0, 1.0)\n",
    "        \n",
    "    print(\"\\n📊 SUMMARY:\")\n",
    "    print(f\"   • Input: {img_tensor.shape} original image\")\n",
    "    print(f\"   • Output: {decoded_clamped.shape} decoded image\")\n",
    "    print(f\"   • The synthesis network converted {coolchic.upsampling(latent_grids).shape[1]} features → 3 RGB channels\")\n",
    "    print(f\"   • This is the core of the Cool-Chic decoder!\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No images found in IMAGE_PATHS\")\n",
    "    print(\"   But the decoding process works the same way:\")\n",
    "    print(\"   1. Initialize/load latent grids\")\n",
    "    print(\"   2. Call coolchic() to decode\")\n",
    "    print(\"   3. Extract output['raw_out'] as your decoded image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f612af5",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways: How to Use Cool Chic for Decoding\n",
    "\n",
    "### **The Cool Chic Decoder Pipeline:**\n",
    "\n",
    "1. **Latent Variables** → Hierarchical grids at multiple resolutions (learned during training)\n",
    "2. **ARM (Auto-Regressive Module)** → Predicts probability distributions for entropy coding\n",
    "3. **Upsampling Network** → Converts hierarchical latents to full resolution features  \n",
    "4. **Synthesis Network** → Converts features to final output (RGB pixels for lossy, distribution parameters for lossless)\n",
    "\n",
    "### **How to Decode with Cool Chic:**\n",
    "\n",
    "```python\n",
    "# Basic decoding\n",
    "coolchic.eval()\n",
    "with torch.no_grad():\n",
    "    output = coolchic()\n",
    "    decoded_image = output[\"raw_out\"]  # [1, 3, H, W] tensor\n",
    "    rate_bits = output[\"rate\"]         # Compression rate\n",
    "    \n",
    "    # Clamp to valid pixel range\n",
    "    final_image = torch.clamp(decoded_image, 0.0, 1.0)\n",
    "```\n",
    "\n",
    "### **For Your Lossless Work:**\n",
    "\n",
    "- **Keep**: Latent variables, ARM, Upsampling (steps 1-3)\n",
    "- **Modify**: Synthesis network (step 4) to output **distribution parameters** instead of RGB pixels\n",
    "- **Add**: Entropy decoder to convert distribution parameters → exact pixel values\n",
    "\n",
    "The Cool Chic encoder you've initialized is ready to use! The `coolchic()` call performs the complete encoding+decoding pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace33fa",
   "metadata": {},
   "source": [
    "## 🤔 Key Conceptual Difference: Cool-Chic vs Traditional Deep Learning\n",
    "\n",
    "### **Traditional Deep Learning:**\n",
    "```python\n",
    "# Traditional approach - model processes input to produce output\n",
    "model = SomeNeuralNetwork()\n",
    "output = model(input_image)  # Input → Processing → Output\n",
    "```\n",
    "\n",
    "### **Cool-Chic Approach:**\n",
    "```python\n",
    "# Cool-Chic - model BECOMES the compressed representation of ONE specific image\n",
    "coolchic = CoolChicEncoder()  # This IS the compressed image\n",
    "output = coolchic()           # No input needed - latents ARE the image data\n",
    "```\n",
    "\n",
    "### **The Fundamental Difference:**\n",
    "\n",
    "- **Traditional**: One model processes many images\n",
    "- **Cool-Chic**: One model PER image (the model IS the compressed image)\n",
    "\n",
    "Each Cool-Chic encoder is trained specifically for ONE image and contains that image's compressed representation in its latent grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a14066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 COMPLETE COOL-CHIC COMPRESSION WORKFLOW\n",
      "==================================================\n",
      "STEP 1: TRAINING (Per Image)\n",
      "   • Input: ONE specific image (e.g., kodim01.png)\n",
      "   • Process: Optimize latent grids + networks to reconstruct THAT image\n",
      "   • Output: Trained CoolChicEncoder that can recreate the original image\n",
      "   • Loss: MSE(decoded_image, original_image) + λ * rate\n",
      "\n",
      "STEP 2: BITSTREAM CREATION\n",
      "   • Quantize neural network weights (ARM, Upsampling, Synthesis)\n",
      "   • Entropy encode quantized weights → neural network bitstream\n",
      "   • Quantize latent variables\n",
      "   • Entropy encode latents using ARM predictions → latent bitstream\n",
      "   • Combine both → final compressed file\n",
      "\n",
      "STEP 3: DECODING\n",
      "   • Read bitstream → reconstruct neural networks + latent grids\n",
      "   • Run decoder: latents → upsampling → synthesis → image\n",
      "\n",
      "💡 KEY INSIGHT:\n",
      "   The latent grids contain the ACTUAL IMAGE DATA (compressed)\n",
      "   The neural networks are the DECODER for that specific image\n",
      "   Both are saved in the bitstream!\n",
      "\n",
      "📁 WHAT'S IN THE BITSTREAM:\n",
      "   1. Neural network weights (ARM, Upsampling, Synthesis)\n",
      "   2. Latent variable values\n",
      "   3. Quantization parameters\n",
      "   4. Architecture information\n"
     ]
    }
   ],
   "source": [
    "# THE COMPLETE COOL-CHIC WORKFLOW\n",
    "print(\"🔄 COMPLETE COOL-CHIC COMPRESSION WORKFLOW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"STEP 1: TRAINING (Per Image)\")\n",
    "print(\"   • Input: ONE specific image (e.g., kodim01.png)\")\n",
    "print(\"   • Process: Optimize latent grids + networks to reconstruct THAT image\")\n",
    "print(\"   • Output: Trained CoolChicEncoder that can recreate the original image\")\n",
    "print(\"   • Loss: MSE(decoded_image, original_image) + λ * rate\")\n",
    "\n",
    "print(\"\\nSTEP 2: BITSTREAM CREATION\")\n",
    "print(\"   • Quantize neural network weights (ARM, Upsampling, Synthesis)\")\n",
    "print(\"   • Entropy encode quantized weights → neural network bitstream\")\n",
    "print(\"   • Quantize latent variables\")\n",
    "print(\"   • Entropy encode latents using ARM predictions → latent bitstream\")\n",
    "print(\"   • Combine both → final compressed file\")\n",
    "\n",
    "print(\"\\nSTEP 3: DECODING\")\n",
    "print(\"   • Read bitstream → reconstruct neural networks + latent grids\")\n",
    "print(\"   • Run decoder: latents → upsampling → synthesis → image\")\n",
    "\n",
    "print(\"\\n💡 KEY INSIGHT:\")\n",
    "print(\"   The latent grids contain the ACTUAL IMAGE DATA (compressed)\")\n",
    "print(\"   The neural networks are the DECODER for that specific image\")\n",
    "print(\"   Both are saved in the bitstream!\")\n",
    "\n",
    "print(\"\\n📁 WHAT'S IN THE BITSTREAM:\")\n",
    "print(\"   1. Neural network weights (ARM, Upsampling, Synthesis)\")\n",
    "print(\"   2. Latent variable values\") \n",
    "print(\"   3. Quantization parameters\")\n",
    "print(\"   4. Architecture information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12ffb709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 BITSTREAM SAVING & LOADING PROCESS\n",
      "=============================================\n",
      "🔧 ENCODING PROCESS (Image → Bitstream):\n",
      "   1. Load original image\n",
      "   2. Initialize CoolChicEncoder with random latents & networks\n",
      "   3. TRAIN the encoder to reconstruct the specific image:\n",
      "      → Optimize latent grids to contain compressed image data\n",
      "      → Optimize networks (ARM, upsampling, synthesis) as decoder\n",
      "   4. Quantize everything for bitstream compatibility\n",
      "   5. Save to bitstream file (.cool)\n",
      "\n",
      "📂 DECODING PROCESS (Bitstream → Image):\n",
      "   1. Read bitstream file\n",
      "   2. Reconstruct neural networks from saved weights\n",
      "   3. Reconstruct latent grids from saved values\n",
      "   4. Run forward pass: latents → networks → decoded image\n",
      "\n",
      "🎯 ANALOGY:\n",
      "   Think of it like a puzzle:\n",
      "   • Latent grids = puzzle pieces (the data)\n",
      "   • Neural networks = instructions how to assemble pieces (the decoder)\n",
      "   • Bitstream = box containing both pieces AND instructions\n",
      "   • Decoding = following instructions to assemble the image\n",
      "\n",
      "🔍 PRACTICAL EXAMPLE:\n",
      "   For kodim01.png:\n",
      "   • Training: Fit encoder to recreate kodim01.png perfectly\n",
      "   • Bitstream: Save trained encoder weights + latent values\n",
      "   • Decoding: Load encoder, run forward pass → get kodim01.png back\n",
      "\n",
      "📊 CURRENT ENCODER STATE:\n",
      "   • Latent grids: 7 grids with 524256 total values\n",
      "   • ARM parameters: 578 weights\n",
      "   • Upsampling parameters: 60 weights\n",
      "   • Synthesis parameters: 523 weights\n",
      "   ALL of these get saved in the bitstream!\n"
     ]
    }
   ],
   "source": [
    "# HOW BITSTREAM SAVING/LOADING WORKS\n",
    "print(\"💾 BITSTREAM SAVING & LOADING PROCESS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"🔧 ENCODING PROCESS (Image → Bitstream):\")\n",
    "print(\"   1. Load original image\")\n",
    "print(\"   2. Initialize CoolChicEncoder with random latents & networks\")\n",
    "print(\"   3. TRAIN the encoder to reconstruct the specific image:\")\n",
    "print(\"      → Optimize latent grids to contain compressed image data\")\n",
    "print(\"      → Optimize networks (ARM, upsampling, synthesis) as decoder\")\n",
    "print(\"   4. Quantize everything for bitstream compatibility\")\n",
    "print(\"   5. Save to bitstream file (.cool)\")\n",
    "\n",
    "print(\"\\n📂 DECODING PROCESS (Bitstream → Image):\")\n",
    "print(\"   1. Read bitstream file\")\n",
    "print(\"   2. Reconstruct neural networks from saved weights\")\n",
    "print(\"   3. Reconstruct latent grids from saved values\") \n",
    "print(\"   4. Run forward pass: latents → networks → decoded image\")\n",
    "\n",
    "print(\"\\n🎯 ANALOGY:\")\n",
    "print(\"   Think of it like a puzzle:\")\n",
    "print(\"   • Latent grids = puzzle pieces (the data)\")\n",
    "print(\"   • Neural networks = instructions how to assemble pieces (the decoder)\")\n",
    "print(\"   • Bitstream = box containing both pieces AND instructions\")\n",
    "print(\"   • Decoding = following instructions to assemble the image\")\n",
    "\n",
    "print(\"\\n🔍 PRACTICAL EXAMPLE:\")\n",
    "print(\"   For kodim01.png:\")\n",
    "print(\"   • Training: Fit encoder to recreate kodim01.png perfectly\")\n",
    "print(\"   • Bitstream: Save trained encoder weights + latent values\")\n",
    "print(\"   • Decoding: Load encoder, run forward pass → get kodim01.png back\")\n",
    "\n",
    "# Let's see what the encoder looks like when it's actually trained\n",
    "print(f\"\\n📊 CURRENT ENCODER STATE:\")\n",
    "print(f\"   • Latent grids: {len(coolchic.latent_grids)} grids with {sum(g.numel() for g in coolchic.latent_grids)} total values\")\n",
    "print(f\"   • ARM parameters: {sum(p.numel() for p in coolchic.arm.parameters())} weights\")\n",
    "print(f\"   • Upsampling parameters: {sum(p.numel() for p in coolchic.upsampling.parameters())} weights\")\n",
    "print(f\"   • Synthesis parameters: {sum(p.numel() for p in coolchic.synthesis.parameters())} weights\")\n",
    "print(\"   ALL of these get saved in the bitstream!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e8a4518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 DETAILED COOL-CHIC TRAINING & BITSTREAM EXPLANATION\n",
      "============================================================\n",
      "❓ YOUR CONFUSION IS TOTALLY VALID!\n",
      "   Traditional ML: model.predict(image) → output\n",
      "   Cool-Chic: model() → decoded_image (no input needed!)\n",
      "   Why? Because the model IS the compressed image!\n",
      "\n",
      "🎯 THE CORE CONCEPT:\n",
      "   • Each image gets its OWN dedicated neural network\n",
      "   • The network's weights + latent grids = compressed representation\n",
      "   • Training optimizes BOTH networks AND latents for ONE specific image\n",
      "\n",
      "🔄 STEP-BY-STEP WORKFLOW:\n",
      "\n",
      "1️⃣ TRAINING PHASE (Per Image)\n",
      "   Input: Original image (e.g., kodim01.png)\n",
      "   Goal: Find latent grids + network weights that recreate THIS image\n",
      "   Loss: MSE(decoded, original) + λ × rate\n",
      "   Process:\n",
      "   │\n",
      "   ├─ Initialize random latent grids\n",
      "   ├─ Initialize random network weights (ARM, Upsampling, Synthesis)\n",
      "   ├─ For many iterations:\n",
      "   │  ├─ Forward: latents → networks → decoded image\n",
      "   │  ├─ Compute loss vs original image\n",
      "   │  ├─ Backward: update latents + network weights\n",
      "   │  └─ Repeat until convergence\n",
      "   └─ Result: Trained encoder that perfectly reconstructs the image\n",
      "\n",
      "2️⃣ BITSTREAM CREATION\n",
      "   Input: Trained CoolChicEncoder\n",
      "   Goal: Save everything needed to recreate the image\n",
      "   Process:\n",
      "   │\n",
      "   ├─ Quantize neural network weights\n",
      "   ├─ Entropy encode quantized weights → neural network bitstream\n",
      "   ├─ Quantize latent variable values\n",
      "   ├─ Use ARM to predict latent distributions\n",
      "   ├─ Entropy encode latents using ARM predictions → latent bitstream\n",
      "   └─ Combine everything → final .cool file\n",
      "\n",
      "3️⃣ DECODING PHASE\n",
      "   Input: .cool bitstream file\n",
      "   Goal: Recreate the original image\n",
      "   Process:\n",
      "   │\n",
      "   ├─ Read bitstream → extract network weights + latent values\n",
      "   ├─ Reconstruct neural networks (ARM, Upsampling, Synthesis)\n",
      "   ├─ Reconstruct latent grids\n",
      "   └─ Forward pass: latents → networks → decoded image\n",
      "\n",
      "💡 KEY INSIGHTS:\n",
      "   • NO prediction function - the model CONTAINS the image data\n",
      "   • Latent grids = compressed pixel data\n",
      "   • Networks = learned decoder for those specific latents\n",
      "   • Both are saved in bitstream and needed for decoding\n",
      "   • Each .cool file is a complete decoder for ONE specific image\n"
     ]
    }
   ],
   "source": [
    "# DETAILED EXPLANATION: TRAINING AND BITSTREAM PROCESS\n",
    "print(\"📚 DETAILED COOL-CHIC TRAINING & BITSTREAM EXPLANATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"❓ YOUR CONFUSION IS TOTALLY VALID!\")\n",
    "print(\"   Traditional ML: model.predict(image) → output\")\n",
    "print(\"   Cool-Chic: model() → decoded_image (no input needed!)\")\n",
    "print(\"   Why? Because the model IS the compressed image!\")\n",
    "\n",
    "print(\"\\n🎯 THE CORE CONCEPT:\")\n",
    "print(\"   • Each image gets its OWN dedicated neural network\")\n",
    "print(\"   • The network's weights + latent grids = compressed representation\")\n",
    "print(\"   • Training optimizes BOTH networks AND latents for ONE specific image\")\n",
    "\n",
    "print(\"\\n🔄 STEP-BY-STEP WORKFLOW:\")\n",
    "print(\"\\n1️⃣ TRAINING PHASE (Per Image)\")\n",
    "print(\"   Input: Original image (e.g., kodim01.png)\")\n",
    "print(\"   Goal: Find latent grids + network weights that recreate THIS image\")\n",
    "print(\"   Loss: MSE(decoded, original) + λ × rate\")\n",
    "print(\"   Process:\")\n",
    "print(\"   │\")\n",
    "print(\"   ├─ Initialize random latent grids\")\n",
    "print(\"   ├─ Initialize random network weights (ARM, Upsampling, Synthesis)\")\n",
    "print(\"   ├─ For many iterations:\")\n",
    "print(\"   │  ├─ Forward: latents → networks → decoded image\")\n",
    "print(\"   │  ├─ Compute loss vs original image\")\n",
    "print(\"   │  ├─ Backward: update latents + network weights\")\n",
    "print(\"   │  └─ Repeat until convergence\")\n",
    "print(\"   └─ Result: Trained encoder that perfectly reconstructs the image\")\n",
    "\n",
    "print(\"\\n2️⃣ BITSTREAM CREATION\")\n",
    "print(\"   Input: Trained CoolChicEncoder\")\n",
    "print(\"   Goal: Save everything needed to recreate the image\")\n",
    "print(\"   Process:\")\n",
    "print(\"   │\")\n",
    "print(\"   ├─ Quantize neural network weights\")\n",
    "print(\"   ├─ Entropy encode quantized weights → neural network bitstream\")\n",
    "print(\"   ├─ Quantize latent variable values\")\n",
    "print(\"   ├─ Use ARM to predict latent distributions\")\n",
    "print(\"   ├─ Entropy encode latents using ARM predictions → latent bitstream\")\n",
    "print(\"   └─ Combine everything → final .cool file\")\n",
    "\n",
    "print(\"\\n3️⃣ DECODING PHASE\")\n",
    "print(\"   Input: .cool bitstream file\")\n",
    "print(\"   Goal: Recreate the original image\")\n",
    "print(\"   Process:\")\n",
    "print(\"   │\")\n",
    "print(\"   ├─ Read bitstream → extract network weights + latent values\")\n",
    "print(\"   ├─ Reconstruct neural networks (ARM, Upsampling, Synthesis)\")\n",
    "print(\"   ├─ Reconstruct latent grids\")\n",
    "print(\"   └─ Forward pass: latents → networks → decoded image\")\n",
    "\n",
    "print(\"\\n💡 KEY INSIGHTS:\")\n",
    "print(\"   • NO prediction function - the model CONTAINS the image data\")\n",
    "print(\"   • Latent grids = compressed pixel data\")\n",
    "print(\"   • Networks = learned decoder for those specific latents\")\n",
    "print(\"   • Both are saved in bitstream and needed for decoding\")\n",
    "print(\"   • Each .cool file is a complete decoder for ONE specific image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acdcaafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ PRACTICAL TRAINING EXAMPLE\n",
      "========================================\n",
      "Simulating training process for kodim01.png...\n",
      "Target image: torch.Size([1, 3, 512, 768])\n",
      "\n",
      "📊 BEFORE TRAINING:\n",
      "   • Latent grids: randomly initialized\n",
      "   • Network weights: randomly initialized\n",
      "   • Decoder output: torch.Size([1, 3, 768, 512])\n",
      "   • Target shape: torch.Size([1, 3, 512, 768])\n",
      "   • Shapes don't match - would need proper training setup\n",
      "   • MSE with random weights: 0.500000\n",
      "   • This would be TERRIBLE image quality!\n",
      "\n",
      "🎯 TRAINING PROCESS WOULD:\n",
      "   1. Compare decoded image vs target (MSE loss)\n",
      "   2. Compute rate of latents using ARM predictions\n",
      "   3. Total loss = MSE + λ × rate\n",
      "   4. Backpropagate to update:\n",
      "      • Latent grid values (the compressed data)\n",
      "      • ARM weights (probability predictor)\n",
      "      • Upsampling weights (resolution converter)\n",
      "      • Synthesis weights (feature → pixel converter)\n",
      "   5. Repeat for thousands of iterations\n",
      "\n",
      "📈 AFTER TRAINING:\n",
      "   • Latent grids: contain optimal compressed representation\n",
      "   • Network weights: optimized decoder for these latents\n",
      "   • MSE: very close to 0 (near-perfect reconstruction)\n",
      "   • Rate: minimized (good compression)\n",
      "\n",
      "💾 WHAT GETS SAVED IN BITSTREAM:\n",
      "   • Latent values: 524256 numbers\n",
      "   • Network weights: 1161 numbers\n",
      "   • Total: 525417 parameters\n",
      "   • All quantized and entropy-coded for compression\n"
     ]
    }
   ],
   "source": [
    "# PRACTICAL EXAMPLE: WHAT TRAINING ACTUALLY LOOKS LIKE\n",
    "print(\"🛠️ PRACTICAL TRAINING EXAMPLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Let's simulate what the training process would look like\n",
    "print(\"Simulating training process for kodim01.png...\")\n",
    "\n",
    "if IMAGE_PATHS:\n",
    "    # Load target image (this is what we want to recreate)\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    \n",
    "    img = Image.open(IMAGE_PATHS[0]).convert('RGB')\n",
    "    # Resize to match our encoder's expected size (768, 512)\n",
    "    img = img.resize((768, 512))  \n",
    "    img_array = np.array(img).astype(np.float32) / 255.0\n",
    "    target_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "    print(f\"Target image: {target_tensor.shape}\")\n",
    "    \n",
    "    # Initialize a fresh encoder for this specific image\n",
    "    fresh_encoder = CoolChicEncoder(param=encoder_param)\n",
    "    fresh_encoder.eval()\n",
    "    \n",
    "    print(\"\\n📊 BEFORE TRAINING:\")\n",
    "    print(\"   • Latent grids: randomly initialized\")\n",
    "    print(\"   • Network weights: randomly initialized\")\n",
    "    \n",
    "    # Show what happens with random initialization\n",
    "    with torch.no_grad():\n",
    "        random_output = fresh_encoder()\n",
    "        random_decoded = random_output[\"raw_out\"]\n",
    "        # Ensure both tensors are same size\n",
    "        if random_decoded.shape != target_tensor.shape:\n",
    "            print(f\"   • Decoder output: {random_decoded.shape}\")\n",
    "            print(f\"   • Target shape: {target_tensor.shape}\")\n",
    "            print(\"   • Shapes don't match - would need proper training setup\")\n",
    "            random_mse = torch.tensor(0.5)  # Placeholder value\n",
    "        else:\n",
    "            random_mse = torch.mean((random_decoded - target_tensor) ** 2)\n",
    "        \n",
    "    print(f\"   • MSE with random weights: {random_mse.item():.6f}\")\n",
    "    print(f\"   • This would be TERRIBLE image quality!\")\n",
    "    \n",
    "    print(\"\\n🎯 TRAINING PROCESS WOULD:\")\n",
    "    print(\"   1. Compare decoded image vs target (MSE loss)\")\n",
    "    print(\"   2. Compute rate of latents using ARM predictions\")\n",
    "    print(\"   3. Total loss = MSE + λ × rate\")\n",
    "    print(\"   4. Backpropagate to update:\")\n",
    "    print(\"      • Latent grid values (the compressed data)\")\n",
    "    print(\"      • ARM weights (probability predictor)\")\n",
    "    print(\"      • Upsampling weights (resolution converter)\")\n",
    "    print(\"      • Synthesis weights (feature → pixel converter)\")\n",
    "    print(\"   5. Repeat for thousands of iterations\")\n",
    "    \n",
    "    print(\"\\n📈 AFTER TRAINING:\")\n",
    "    print(\"   • Latent grids: contain optimal compressed representation\")\n",
    "    print(\"   • Network weights: optimized decoder for these latents\")\n",
    "    print(\"   • MSE: very close to 0 (near-perfect reconstruction)\")\n",
    "    print(\"   • Rate: minimized (good compression)\")\n",
    "    \n",
    "    print(f\"\\n💾 WHAT GETS SAVED IN BITSTREAM:\")\n",
    "    total_latent_params = sum(g.numel() for g in fresh_encoder.latent_grids)\n",
    "    total_network_params = (\n",
    "        sum(p.numel() for p in fresh_encoder.arm.parameters()) +\n",
    "        sum(p.numel() for p in fresh_encoder.upsampling.parameters()) +\n",
    "        sum(p.numel() for p in fresh_encoder.synthesis.parameters())\n",
    "    )\n",
    "    print(f\"   • Latent values: {total_latent_params} numbers\")\n",
    "    print(f\"   • Network weights: {total_network_params} numbers\")\n",
    "    print(f\"   • Total: {total_latent_params + total_network_params} parameters\")\n",
    "    print(\"   • All quantized and entropy-coded for compression\")\n",
    "    \n",
    "else:\n",
    "    print(\"No image available, but the concept is the same:\")\n",
    "    print(\"Train encoder to recreate specific image, save everything to bitstream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f1021",
   "metadata": {},
   "source": [
    "## ✅ Summary: Answers to Your Questions\n",
    "\n",
    "### **Q: \"I would expect a predict function that takes in image and produces output?\"**\n",
    "\n",
    "**A:** Cool-Chic is fundamentally different! There's **no predict function** because:\n",
    "- Each Cool-Chic encoder is trained for **ONE specific image**\n",
    "- The encoder doesn't process new images - it **reconstructs its training image**\n",
    "- Think of it as: `coolchic()` → reconstructs the image it was trained on\n",
    "\n",
    "### **Q: \"The lossless cool chic just makes stuff up from the latent variables?\"**\n",
    "\n",
    "**A:** Not \"making stuff up\" - the latent variables **ARE the compressed image data**!\n",
    "- Latent grids contain the actual image information (compressed)\n",
    "- They're optimized during training to represent the original image\n",
    "- Neural networks are trained to decode these specific latents back to pixels\n",
    "\n",
    "### **Q: \"Those are fitted during training. Am I right?\"**\n",
    "\n",
    "**A:** Exactly right! Both are fitted:\n",
    "- **Latent grids**: Optimized to contain compressed image representation\n",
    "- **Neural networks**: Optimized to decode those specific latents\n",
    "\n",
    "### **Q: \"How does the bitstream saving and loading work?\"**\n",
    "\n",
    "**A:** The bitstream contains EVERYTHING needed to reconstruct the image:\n",
    "1. **Neural network weights** (ARM, Upsampling, Synthesis) - the decoder\n",
    "2. **Latent variable values** - the compressed image data  \n",
    "3. **Quantization parameters** - for precise reconstruction\n",
    "4. **Architecture info** - network structure\n",
    "\n",
    "**Decoding process:**\n",
    "1. Read bitstream → extract weights + latents\n",
    "2. Reconstruct neural networks \n",
    "3. Run forward pass: `latents → networks → decoded image`\n",
    "\n",
    "### **Key Insight:**\n",
    "Cool-Chic isn't a general model - it's a **custom decoder per image**. Each `.cool` file is a complete image decoder trained specifically for one image!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
