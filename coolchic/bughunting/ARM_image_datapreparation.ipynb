{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a73e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask which is used during printing, order is H, W\n",
    "print_mask = [\n",
    "                                [-3, 0],\n",
    "                                [-2, 0],\n",
    "                      [-1, -1], [-1, 0], [-1, 1],\n",
    "    [0, -3], [0, -2], [0,  -1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e32bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def _get_non_zero_pixel_ctx_index(dim_arm: int) -> torch.Tensor:\n",
    "    \"\"\"Generate the relative index of the context pixel with respect to the\n",
    "    actual pixel being decoded.\n",
    "\n",
    "    1D tensor containing the indices of the non zero context. This corresponds to the one\n",
    "    in the pattern above. This allows to use the index_select function, which is significantly\n",
    "    faster than usual indexing.\n",
    "\n",
    "    0   1   2   3   4   5   6   7   8\n",
    "    9   10  11  12  13  14  15  16  17\n",
    "    18  19  20  21  22  23  24  25  26\n",
    "    27  28  29  30  31  32  33  34  35\n",
    "    36  37  38  39  *   x   x   x   x\n",
    "    x   x   x   x   x   x   x   x   x\n",
    "    x   x   x   x   x   x   x   x   x\n",
    "    x   x   x   x   x   x   x   x   x\n",
    "    x   x   x   x   x   x   x   x   x\n",
    "\n",
    "\n",
    "    Args:\n",
    "        dim_arm (int): Number of context pixels\n",
    "\n",
    "    Returns:\n",
    "        Tensor: 1D tensor with the flattened index of the context pixels.\n",
    "    \"\"\"\n",
    "    # fmt: off\n",
    "    if dim_arm == 8:\n",
    "        return torch.tensor(\n",
    "            [            13,\n",
    "                         22,\n",
    "                     30, 31, 32,\n",
    "             37, 38, 39, #\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    raise ValueError(f\"dim_arm {dim_arm} not supported.\")\n",
    "\n",
    "\n",
    "def _get_neighbor(\n",
    "    x: torch.Tensor, mask_size: int, non_zero_pixel_ctx_idx: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Use the unfold function to extract the neighbors of each pixel in x.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): [1, 1, H, W] feature map from which we wish to extract the\n",
    "            neighbors\n",
    "        mask_size (int): Virtual size of the kernel around the current coded latent.\n",
    "            mask_size = 2 * n_ctx_rowcol - 1\n",
    "        non_zero_pixel_ctx_idx (Tensor): [N] 1D tensor containing the indices\n",
    "            of the non zero context pixels (i.e. floor(N ** 2 / 2) - 1).\n",
    "            It looks like: [0, 1, ..., floor(N ** 2 / 2) - 1].\n",
    "            This allows to use the index_select function, which is significantly\n",
    "            faster than usual indexing.\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: [H * W, floor(N ** 2 / 2) - 1] the spatial neighbors\n",
    "            the floor(N ** 2 / 2) - 1 neighbors of each H * W pixels.\n",
    "    \"\"\"\n",
    "    pad = int((mask_size - 1) / 2)\n",
    "    x_pad = F.pad(x, (pad, pad, pad, pad), mode=\"constant\", value=0.0)\n",
    "\n",
    "    # Shape of x_unfold is [B, C, H, W, mask_size, mask_size] --> [B * C * H * W, mask_size * mask_size]\n",
    "    # reshape is faster than einops.rearrange\n",
    "    x_unfold = (\n",
    "        x_pad.unfold(2, mask_size, step=1)\n",
    "        .unfold(3, mask_size, step=1)\n",
    "        .reshape(-1, mask_size * mask_size)\n",
    "    )\n",
    "\n",
    "    # Convert x_unfold to a 2D tensor: [Number of pixels, all neighbors]\n",
    "    # This is slower than reshape above\n",
    "    # x_unfold = rearrange(\n",
    "    #     x_unfold,\n",
    "    #     'b c h w mask_h mask_w -> (b c h w) (mask_h mask_w)'\n",
    "    # )\n",
    "\n",
    "    # Select the pixels for which the mask is not zero\n",
    "    # For a N x N mask, select only the first (N x N - 1) / 2 pixels\n",
    "    # (those which aren't null)\n",
    "    neighbor = torch.index_select(x_unfold, dim=1, index=non_zero_pixel_ctx_idx)\n",
    "    return neighbor\n",
    "\n",
    "\n",
    "def prepare_inputs(image: torch.Tensor, raw_synth_out: torch.Tensor):\n",
    "    contexts = []\n",
    "    synthesis_out_params_per_channel = [2, 3, 4]\n",
    "    mask_size = 9\n",
    "    context_size = 8\n",
    "    non_zero_image_arm_ctx_index = _get_non_zero_pixel_ctx_index(context_size)\n",
    "\n",
    "    assert len(synthesis_out_params_per_channel) == image.shape[1], (\n",
    "        \"Number of channels in image and synthesis_out_params_per_channel \"\n",
    "        \"must be equal.\"\n",
    "    )\n",
    "\n",
    "    # First get contexts for all channels in the image\n",
    "    # Use loop as _get_neighbor supports only [1, 1, H, W] input shape\n",
    "    for channel_idx in range(len(synthesis_out_params_per_channel)):\n",
    "        contexts.append(\n",
    "            _get_neighbor(\n",
    "                image[:, channel_idx : channel_idx + 1, :, :],\n",
    "                mask_size,\n",
    "                non_zero_image_arm_ctx_index,\n",
    "            )\n",
    "        )\n",
    "    # Now concatenate the num_channels [H *W, context_size] shaped image contexts\n",
    "    # into [H *W, context_size * num_channels]\n",
    "    flat_image_context = torch.stack(contexts, dim=2).reshape(\n",
    "        (image.shape[2] * image.shape[3], -1)\n",
    "    )\n",
    "\n",
    "    # Add synthesis output and already decoded channels information\n",
    "    prepared_inputs = []\n",
    "    for channel_idx in range(len(synthesis_out_params_per_channel)):\n",
    "        prepared_inputs.append(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    flat_image_context,\n",
    "                    # synthesis output has shape [1, C, H, W], we want [H*W, C]\n",
    "                    raw_synth_out.permute(0, 2, 3, 1).reshape(\n",
    "                        -1, sum(synthesis_out_params_per_channel)\n",
    "                    ),\n",
    "                    # append the couple of already decoded channels\n",
    "                    (\n",
    "                        image[:, :channel_idx]\n",
    "                        .permute(0, 2, 3, 1)\n",
    "                        .reshape(\n",
    "                            -1,\n",
    "                            channel_idx,\n",
    "                        )\n",
    "                        if channel_idx > 0\n",
    "                        else torch.empty(\n",
    "                            image.shape[2] * image.shape[3],\n",
    "                            0,\n",
    "                            dtype=image.dtype,\n",
    "                            device=image.device,\n",
    "                        )\n",
    "                    ),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        )\n",
    "    return prepared_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69058b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = np.arange(3 * 81, dtype=int).reshape((1, 3, 9, 9))\n",
    "C, H, W = 3, 4, 5\n",
    "R = 9\n",
    "CHW = C * H * W\n",
    "image = np.array(\n",
    "    [\n",
    "        [\n",
    "            [[h * W * C + w * C + c for w in range(W)] for h in range(H)]\n",
    "            for c in range(C)\n",
    "        ]\n",
    "    ],\n",
    "    dtype=int,\n",
    ")\n",
    "raw_synth_out = np.arange(R * H * W, dtype=int).reshape((1, H, W, R)).transpose(0, 3, 1, 2) + CHW\n",
    "image = torch.tensor(image)\n",
    "raw_synth_out = torch.tensor(raw_synth_out)\n",
    "\n",
    "# print(image)\n",
    "# print()\n",
    "# print(raw_synth_out)\n",
    "\n",
    "prepared_inputs = prepare_inputs(image, raw_synth_out)\n",
    "r, g, b = prepared_inputs\n",
    "# print(r.shape)\n",
    "# print(g.shape)\n",
    "# print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebf03e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel 0:\n",
      "   0    3    6    9   x\n",
      "  15   18   21   24   x\n",
      "  30   33   36   x   x\n",
      "  45   x   x   x   57\n",
      "\n",
      "Channel 1:\n",
      "   1    4    7   10   x\n",
      "  16   19   22   25   x\n",
      "  31   34   37   x   x\n",
      "  46   x   x   x   58\n",
      "\n",
      "Channel 2:\n",
      "   2    5    8   11   x\n",
      "  17   20   23   26   x\n",
      "  32   35   38   x   x\n",
      "  47   x   x   x   59\n",
      "\n",
      "ind:  0, [    0    1    2 ],     0    1\n",
      "ind:  1, [    3    4    5 ],     3    4\n",
      "ind:  2, [    6    7    8 ],     6    7\n",
      "ind:  3, [    9   10   11 ],     9   10\n",
      "ind:  4, [   12   13   14 ],    12   13\n",
      "ind:  5, [   15   16   17 ],    15   16\n",
      "ind:  6, [   18   19   20 ],    18   19\n",
      "ind:  7, [   21   22   23 ],    21   22\n",
      "ind:  8, [   24   25   26 ],    24   25\n",
      "ind:  9, [   27   28   29 ],    27   28\n",
      "ind: 10, [   30   31   32 ],    30   31\n",
      "ind: 11, [   33   34   35 ],    33   34\n",
      "ind: 12, [   36   37   38 ],    36   37\n",
      "ind: 13, [   39   40   41 ],    39   40\n",
      "ind: 14, [   42   43   44 ],    42   43\n",
      "ind: 15, [   45   46   47 ],    45   46\n",
      "ind: 16, [   48   49   50 ],    48   49\n",
      "ind: 17, [   51   52   53 ],    51   52\n",
      "ind: 18, [   54   55   56 ],    54   55\n",
      "ind: 19, [   57   58   59 ],    57   58\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(image, torch.Tensor)\n",
    "\n",
    "def f_num(num, width=4):\n",
    "    \"\"\"Formats a number to `width` characters\"\"\"\n",
    "    formatted = str(int(num))\n",
    "    return formatted.rjust(width)\n",
    "\n",
    "\n",
    "def group_triplets(a: list):\n",
    "    \"\"\"Groups a list into triplets for showing pixel values\"\"\"\n",
    "    if len(a) % 3 != 0:\n",
    "        return a\n",
    "    grouped = []\n",
    "    for i in range(0, len(a), 3):\n",
    "        grouped.append(\"\".join(a[i : i + 3]).replace(\" \", \"_\"))\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def visualize_context(image: torch.Tensor, mask: list, point: list = [3, 3]):\n",
    "    _, C, H, W = image.shape\n",
    "    im_vals = [[[f_num(image[0, c, h, w].tolist()) for w in range(W)] for h in range(H)] for c in range(C)]\n",
    "    \n",
    "    for dx, dy in mask:\n",
    "        x, y = point[0] + dx, point[1] + dy\n",
    "        if 0 <= x < H and 0 <= y < W:\n",
    "            for c in range(C):\n",
    "                im_vals[c][x][y] = f\"  x\"\n",
    "    \n",
    "    for c in range(C):\n",
    "        print(f\"Channel {c}:\")\n",
    "        for h in range(H):\n",
    "            print(\" \".join(im_vals[c][h]))\n",
    "        print()\n",
    "\n",
    "visualize_context(image, print_mask, point = [3,4])\n",
    "\n",
    "# 0 = red\n",
    "# 1 = green\n",
    "# 2 = blue\n",
    "chosen_channel = 2\n",
    "working_with = prepared_inputs[chosen_channel]\n",
    "check_bounds = [[0, 24], [24, 24 + 9], [24 + 9, 24 + 9 + 10]]\n",
    "# 0 = context pixels\n",
    "# 1 = raw_synth_out\n",
    "# 2 = intra pixel context (red for green, red, green for blue)\n",
    "check_part_index = 2\n",
    "working_with = working_with[\n",
    "    :, check_bounds[check_part_index][0] : check_bounds[check_part_index][1]\n",
    "]\n",
    "for ind in range(H * W):\n",
    "    # if ind != 18:\n",
    "    #     continue\n",
    "    tmp = list(map(f_num, working_with[ind].detach().cpu().numpy()))\n",
    "    tmp = group_triplets(tmp)\n",
    "    img_vals_at_pixel = list(\n",
    "        map(f_num, image[0, :, ind // W, ind % W].tolist())\n",
    "    )\n",
    "    print(f\"ind: {str(ind).rjust(2)}, [\", *img_vals_at_pixel, \"], \", *tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3fdae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  1,  2],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  1,  2,  3,  4,  5],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,\n",
      "          3,  4,  5,  6,  7,  8],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  4,  5,\n",
      "          6,  7,  8,  9, 10, 11],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  3,  4,  5,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  1,  2,  3,  4,  5,  6,  7,  8,  0,  0,  0,\n",
      "          0,  0,  0, 15, 16, 17],\n",
      "        [ 0,  0,  0,  0,  0,  0,  3,  4,  5,  6,  7,  8,  9, 10, 11,  0,  0,  0,\n",
      "         15, 16, 17, 18, 19, 20],\n",
      "        [ 0,  0,  0,  0,  0,  0,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23],\n",
      "        [ 0,  0,  0,  0,  0,  0,  9, 10, 11, 12, 13, 14,  0,  0,  0, 18, 19, 20,\n",
      "         21, 22, 23, 24, 25, 26],\n",
      "        [ 0,  0,  0,  0,  1,  2,  0,  0,  0, 15, 16, 17, 18, 19, 20,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  3,  4,  5, 15, 16, 17, 18, 19, 20, 21, 22, 23,  0,  0,  0,\n",
      "          0,  0,  0, 30, 31, 32],\n",
      "        [ 0,  0,  0,  6,  7,  8, 18, 19, 20, 21, 22, 23, 24, 25, 26,  0,  0,  0,\n",
      "         30, 31, 32, 33, 34, 35],\n",
      "        [ 0,  0,  0,  9, 10, 11, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
      "         33, 34, 35, 36, 37, 38],\n",
      "        [ 0,  0,  0, 12, 13, 14, 24, 25, 26, 27, 28, 29,  0,  0,  0, 33, 34, 35,\n",
      "         36, 37, 38, 39, 40, 41],\n",
      "        [ 0,  1,  2, 15, 16, 17,  0,  0,  0, 30, 31, 32, 33, 34, 35,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0],\n",
      "        [ 3,  4,  5, 18, 19, 20, 30, 31, 32, 33, 34, 35, 36, 37, 38,  0,  0,  0,\n",
      "          0,  0,  0, 45, 46, 47],\n",
      "        [ 6,  7,  8, 21, 22, 23, 33, 34, 35, 36, 37, 38, 39, 40, 41,  0,  0,  0,\n",
      "         45, 46, 47, 48, 49, 50],\n",
      "        [ 9, 10, 11, 24, 25, 26, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\n",
      "         48, 49, 50, 51, 52, 53],\n",
      "        [12, 13, 14, 27, 28, 29, 39, 40, 41, 42, 43, 44,  0,  0,  0, 48, 49, 50,\n",
      "         51, 52, 53, 54, 55, 56]])\n",
      "torch.Size([20, 24])\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(image, torch.Tensor)\n",
    "\n",
    "contexts = []\n",
    "non_zero_image_arm_ctx_index = _get_non_zero_pixel_ctx_index(8)\n",
    "for channel_idx in range(3):\n",
    "    contexts.append(\n",
    "        _get_neighbor(\n",
    "            image[:, channel_idx : channel_idx + 1, :, :],\n",
    "            9,\n",
    "            non_zero_image_arm_ctx_index,\n",
    "        )\n",
    "    )\n",
    "# Now concatenate the num_channels [H *W, context_size] shaped image contexts\n",
    "# into [H *W, context_size * num_channels]\n",
    "# print(contexts)\n",
    "# print(contexts[0].shape)\n",
    "flat_image_context = torch.stack(contexts, dim=2).reshape((H*W, -1))\n",
    "print(flat_image_context)\n",
    "print(flat_image_context.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
