{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54f4693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.getcwd())\n",
    "# from lossless.component.core.arm_image import ImageArm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Software Name: Cool-Chic\n",
    "# SPDX-FileCopyrightText: Copyright (c) 2023-2025 Orange\n",
    "# SPDX-License-Identifier: BSD 3-Clause \"New\"\n",
    "#\n",
    "# This software is distributed under the BSD-3-Clause license.\n",
    "#\n",
    "# Authors: see CONTRIBUTORS.md\n",
    "\n",
    "\n",
    "from typing import OrderedDict, Tuple\n",
    "from lossless.component.core.arm import (\n",
    "    _get_neighbor,\n",
    "    _get_non_zero_pixel_ctx_index,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "from torch import Tensor, nn  # , index_select\n",
    "from lossless.component.core.arm import ArmLinear, _get_neighbor\n",
    "\n",
    "\n",
    "class ImageArm(nn.Module):\n",
    "    \"\"\"Instantiate an autoregressive probability module, modelling the\n",
    "    conditional distribution :math:`p_{\\\\psi}(\\\\hat{y}_i \\\\mid\n",
    "    \\\\mathbf{c}_i)` of a (quantized) latent pixel :math:`\\\\hat{y}_i`,\n",
    "    conditioned on neighboring already decoded context pixels\n",
    "    :math:`\\\\mathbf{c}_i \\in \\\\mathbb{Z}^C`, where :math:`C` denotes the\n",
    "    number of context pixels.\n",
    "\n",
    "    The distribution :math:`p_{\\\\psi}` is assumed to follow a Laplace\n",
    "    distribution, parameterized by an expectation :math:`\\\\mu` and a scale\n",
    "    :math:`b`, where the scale and the variance :math:`\\\\sigma^2` are\n",
    "    related as follows :math:`\\\\sigma^2 = 2 b ^2`.\n",
    "\n",
    "    The parameters of the Laplace distribution for a given latent pixel\n",
    "    :math:`\\\\hat{y}_i` are obtained by passing its context pixels\n",
    "    :math:`\\\\mathbf{c}_i` through an MLP :math:`f_{\\\\psi}`:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        p_{\\\\psi}(\\\\hat{y}_i \\\\mid \\\\mathbf{c}_i) \\sim \\mathcal{L}(\\\\mu_i,\n",
    "        b_i), \\\\text{ where } \\\\mu_i, b_i = f_{\\\\psi}(\\\\mathbf{c}_i).\n",
    "\n",
    "    .. attention::\n",
    "\n",
    "        The MLP :math:`f_{\\\\psi}` has a few constraint on its architecture:\n",
    "\n",
    "        * The width of all hidden layers (i.e. the output of all layers except\n",
    "          the final one) are identical to the number of pixel contexts\n",
    "          :math:`C`;\n",
    "\n",
    "        * All layers except the last one are residual layers, followed by a\n",
    "          ``ReLU`` non-linearity;\n",
    "\n",
    "        * :math:`C` must be at a multiple of 8.\n",
    "\n",
    "    The MLP :math:`f_{\\\\psi}` is made of custom Linear layers instantiated\n",
    "    from the ``ArmLinear`` class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_size: int = 8,\n",
    "        n_hidden_layers: int = 2,\n",
    "        hidden_layer_dim: int = 6,\n",
    "        synthesis_out_params_per_channel: list[int] = [2, 3, 4],\n",
    "        channel_separation: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context_size: Number of pixels to take into context\n",
    "            n_hidden_layers: Number of hidden layers. Set it to 0 for\n",
    "                a linear ARM.\n",
    "            hidden_layer_dim: Size of hidden layer output\n",
    "            synthesis_out_params_per_channel: How many values from\n",
    "                synthesis_out does each channel consume and produce\n",
    "                (residual=True)\n",
    "            channel_separation: Use separate networks for each channel. Also use\n",
    "                information from previous channels appended to the context.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert context_size % 8 == 0, (\n",
    "            f\"ARM context size and hidden layer dimension must be \"\n",
    "            f\"a multiple of 8. Found {context_size}.\"\n",
    "        )\n",
    "        self.context_size = context_size\n",
    "        self.synthesis_out_params_per_channel = synthesis_out_params_per_channel\n",
    "        self.channel_separation = channel_separation\n",
    "\n",
    "        if not channel_separation:\n",
    "            raise NotImplementedError(\n",
    "                \"Non channel-separated ARM is not implemented yet.\"\n",
    "            )\n",
    "\n",
    "        # ======================== Construct the MLPs ======================== #\n",
    "        self.model_layers = [\n",
    "            nn.ModuleList()\n",
    "            for _ in range(len(self.synthesis_out_params_per_channel))\n",
    "        ]\n",
    "        self.models = nn.ModuleList(\n",
    "            nn.Sequential()\n",
    "            for _ in range(len(self.synthesis_out_params_per_channel))\n",
    "        )\n",
    "        for channel_idx, output_dim in enumerate(\n",
    "            self.synthesis_out_params_per_channel\n",
    "        ):\n",
    "            self.model_layers[channel_idx].append(\n",
    "                ArmLinear(\n",
    "                    context_size\n",
    "                    * len(\n",
    "                        self.synthesis_out_params_per_channel\n",
    "                    )  # context size * num_channels\n",
    "                    + sum(\n",
    "                        self.synthesis_out_params_per_channel\n",
    "                    )  # we can use all information from synthesis output\n",
    "                    + channel_idx,  # extra information from already decoded channels for current pixel\n",
    "                    # ,\n",
    "                    hidden_layer_dim,\n",
    "                    residual=False,\n",
    "                )\n",
    "            )\n",
    "            self.model_layers[channel_idx].append(nn.ReLU())\n",
    "\n",
    "            # Construct the hidden layer(s)\n",
    "            for _ in range(n_hidden_layers - 1):\n",
    "                self.model_layers[channel_idx].append(\n",
    "                    ArmLinear(hidden_layer_dim, hidden_layer_dim, residual=True)\n",
    "                )\n",
    "                self.model_layers[channel_idx].append(nn.ReLU())\n",
    "            # Construct the output layer. It always has output_dim 2*outputs\n",
    "            # since we use the second half for gating\n",
    "            self.model_layers[channel_idx].append(\n",
    "                ArmLinear(hidden_layer_dim, output_dim * 2, residual=False)\n",
    "            )\n",
    "            self.models[channel_idx] = nn.Sequential(\n",
    "                *self.model_layers[channel_idx]\n",
    "            )\n",
    "\n",
    "        self.mask_size = 9\n",
    "        self.register_buffer(\n",
    "            \"non_zero_image_arm_ctx_index\",\n",
    "            _get_non_zero_pixel_ctx_index(self.context_size),\n",
    "            persistent=False,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs(self, image: Tensor, raw_synth_out: Tensor):\n",
    "        contexts = []\n",
    "        assert len(self.synthesis_out_params_per_channel) == image.shape[1], (\n",
    "            \"Number of channels in image and synthesis_out_params_per_channel \"\n",
    "            \"must be equal.\"\n",
    "        )\n",
    "\n",
    "        # First get contexts for all channels in the image\n",
    "        # Use loop as _get_neighbor supports only [1, 1, H, W] input shape\n",
    "        for channel_idx in range(len(self.synthesis_out_params_per_channel)):\n",
    "            contexts.append(\n",
    "                _get_neighbor(\n",
    "                    image[:, channel_idx : channel_idx + 1, :, :],\n",
    "                    self.mask_size,\n",
    "                    self.non_zero_image_arm_ctx_index,  # type: ignore\n",
    "                )\n",
    "            )\n",
    "        # Now concatenate the num_channels [H *W, context_size] shaped image contexts\n",
    "        # into [H *W, context_size * num_channels]\n",
    "        flat_image_context = torch.stack(contexts, dim=2).reshape(\n",
    "            (image.shape[2] * image.shape[3], -1)\n",
    "        )\n",
    "\n",
    "        # Add synthesis output and already decoded channels information\n",
    "        prepared_inputs = []\n",
    "        for channel_idx in range(len(self.synthesis_out_params_per_channel)):\n",
    "            prepared_inputs.append(\n",
    "                torch.cat(\n",
    "                    [\n",
    "                        flat_image_context,\n",
    "                        # synthesis output has shape [1, C, H, W], we want [H*W, C]\n",
    "                        raw_synth_out.permute(0, 2, 3, 1).reshape(\n",
    "                            -1, sum(self.synthesis_out_params_per_channel)\n",
    "                        ),\n",
    "                        # append the couple of already decoded channels\n",
    "                        (\n",
    "                            image[:, :channel_idx]\n",
    "                            .permute(0, 2, 3, 1)\n",
    "                            .reshape(\n",
    "                                -1,\n",
    "                                channel_idx,\n",
    "                            )\n",
    "                            if channel_idx > 0\n",
    "                            else torch.empty(\n",
    "                                image.shape[2] * image.shape[3],\n",
    "                                0,\n",
    "                                dtype=image.dtype,\n",
    "                                device=image.device,\n",
    "                                requires_grad=True,\n",
    "                            )\n",
    "                        ),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            )\n",
    "        return prepared_inputs\n",
    "\n",
    "    def forward(self, x: Tensor, synthesis_proba: Tensor) -> Tensor:\n",
    "        \"\"\"Perform the auto-regressive module (ARM) forward pass. The ARM takes\n",
    "        as input a tensor of shape :math:`[B, C]` i.e. :math:`B` contexts with\n",
    "        :math:`C` context pixels. ARM outputs :math:`[B, 2]` values correspond\n",
    "        to :math:`\\\\mu, b` for each of the :math:`B` input pixels.\n",
    "\n",
    "        .. warning::\n",
    "\n",
    "            Note that the ARM expects input to be flattened i.e. spatial\n",
    "            dimensions :math:`H, W` are collapsed into a single batch-like\n",
    "            dimension :math:`B = HW`, leading to an input of shape\n",
    "            :math:`[B, C]`, gathering the :math:`C` contexts for each of the\n",
    "            :math:`B` pixels to model.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            The ARM MLP does not output directly the scale :math:`b`. Denoting\n",
    "            :math:`s` the raw output of the MLP, the scale is obtained as\n",
    "            follows:\n",
    "\n",
    "            .. math::\n",
    "\n",
    "                b = e^{x - 4}\n",
    "\n",
    "        Args:\n",
    "            x: Concatenation of all input contexts\n",
    "                :math:`\\\\mathbf{c}_i`. Tensor of shape :math:`[B, C]`.\n",
    "\n",
    "        Returns:\n",
    "            Concatenation of all Laplace distributions param :math:`\\\\mu, b`.\n",
    "            Tensor of shape :math:([B]). Also return the *log scale*\n",
    "            :math:`s` as described above. Tensor of shape :math:`(B)`\n",
    "        \"\"\"\n",
    "        prepared_inputs = self.prepare_inputs(x, synthesis_proba)\n",
    "\n",
    "        cutoffs = [\n",
    "            sum(self.synthesis_out_params_per_channel[:i])\n",
    "            for i in range(len(self.synthesis_out_params_per_channel) + 1)\n",
    "        ]\n",
    "        out_probas_param = []\n",
    "        for channel in range(len(self.synthesis_out_params_per_channel)):\n",
    "            raw_outs = self.models[channel](prepared_inputs[channel])\n",
    "            raw_proba_param, gate = raw_outs.chunk(2, dim=1)\n",
    "            out_probas_param.append(\n",
    "                synthesis_proba.permute(0, 2, 3, 1).reshape(\n",
    "                    -1, sum(self.synthesis_out_params_per_channel)\n",
    "                )[:, cutoffs[channel] : cutoffs[channel + 1]]\n",
    "                # + raw_proba_param * torch.sigmoid(gate)\n",
    "            )\n",
    "        out_proba_param = torch.cat(out_probas_param, dim=1)\n",
    "        reshaped_image_arm_out = out_proba_param.view(\n",
    "            synthesis_proba.shape[0],\n",
    "            synthesis_proba.shape[2],\n",
    "            synthesis_proba.shape[3],\n",
    "            synthesis_proba.shape[1],\n",
    "        ).permute(0, 3, 1, 2)\n",
    "\n",
    "        return reshaped_image_arm_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3df7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 4, 5])\n",
      "synth_out:\n",
      "tensor([[ 60.,  61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.],\n",
      "        [ 69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.],\n",
      "        [ 78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.],\n",
      "        [ 87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.],\n",
      "        [ 96.,  97.,  98.,  99., 100., 101., 102., 103., 104.],\n",
      "        [105., 106., 107., 108., 109., 110., 111., 112., 113.],\n",
      "        [114., 115., 116., 117., 118., 119., 120., 121., 122.],\n",
      "        [123., 124., 125., 126., 127., 128., 129., 130., 131.],\n",
      "        [132., 133., 134., 135., 136., 137., 138., 139., 140.],\n",
      "        [141., 142., 143., 144., 145., 146., 147., 148., 149.],\n",
      "        [150., 151., 152., 153., 154., 155., 156., 157., 158.],\n",
      "        [159., 160., 161., 162., 163., 164., 165., 166., 167.],\n",
      "        [168., 169., 170., 171., 172., 173., 174., 175., 176.],\n",
      "        [177., 178., 179., 180., 181., 182., 183., 184., 185.],\n",
      "        [186., 187., 188., 189., 190., 191., 192., 193., 194.],\n",
      "        [195., 196., 197., 198., 199., 200., 201., 202., 203.],\n",
      "        [204., 205., 206., 207., 208., 209., 210., 211., 212.],\n",
      "        [213., 214., 215., 216., 217., 218., 219., 220., 221.],\n",
      "        [222., 223., 224., 225., 226., 227., 228., 229., 230.],\n",
      "        [231., 232., 233., 234., 235., 236., 237., 238., 239.]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "C, H, W = 3, 4, 5\n",
    "R = 9\n",
    "CHW = C * H * W\n",
    "image = np.array(\n",
    "    [\n",
    "        [\n",
    "            [[h * W * C + w * C + c for w in range(W)] for h in range(H)]\n",
    "            for c in range(C)\n",
    "        ]\n",
    "    ],\n",
    "    dtype=int,\n",
    ")\n",
    "raw_synth_out = np.arange(R * H * W, dtype=int).reshape((1, H, W, R)).transpose(0, 3, 1, 2) + CHW\n",
    "image = torch.tensor(image, dtype=torch.float32).to(device)\n",
    "raw_synth_out = torch.tensor(raw_synth_out, dtype=torch.float32).to(device)\n",
    "\n",
    "image_arm = ImageArm().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    synth_out = image_arm.forward(image, raw_synth_out)\n",
    "    print(synth_out.shape)\n",
    "    print(\"synth_out:\", synth_out[0].reshape(9, -1).permute(1, 0), sep=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cool_chic_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
