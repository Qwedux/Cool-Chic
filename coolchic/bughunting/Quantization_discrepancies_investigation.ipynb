{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f76a4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using image in RGB color space\n",
      "Using image ARM: True\n",
      "Using encoder gain: 16\n",
      "Using multi-region image ARM: True\n",
      "Using color regression: False\n",
      "Total MAC per pixel: 1694.3125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"bughunting\":\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "    sys.path.append(os.getcwd())\n",
    "import numpy as np\n",
    "import torch\n",
    "from lossless.component.coolchic import CoolChicEncoder, CoolChicEncoderParameter\n",
    "from lossless.configs.config import args, str_args\n",
    "from lossless.training.loss import loss_function\n",
    "from lossless.training.manager import ImageEncoderManager\n",
    "from lossless.util.command_line_args_loading import load_args\n",
    "from lossless.util.image_loading import load_image_as_tensor\n",
    "from lossless.util.logger import TrainingLogger\n",
    "from lossless.util.parsecli import change_n_out_synth, get_coolchic_param_from_args\n",
    "from lossless.util.color_transform import RGBBitdepths\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# ==========================================================================================\n",
    "# LOAD COMMAND LINE ARGS AND IMAGE\n",
    "# ==========================================================================================\n",
    "command_line_args = load_args()\n",
    "command_line_args.color_space = \"RGB\"\n",
    "im_path = args[\"input\"][command_line_args.image_index]\n",
    "im_tensor, colorspace_bitdepths = load_image_as_tensor(\n",
    "    im_path, device=\"cuda:0\", color_space=command_line_args.color_space\n",
    ")\n",
    "# ==========================================================================================\n",
    "# LOAD PRESETS, COOLCHIC PARAMETERS\n",
    "# ==========================================================================================\n",
    "image_encoder_manager = ImageEncoderManager(\n",
    "    preset_name=args[\"preset\"], colorspace_bitdepths=colorspace_bitdepths\n",
    ")\n",
    "\n",
    "\n",
    "encoder_param = CoolChicEncoderParameter(**get_coolchic_param_from_args(args, \"lossless\"))\n",
    "encoder_param.encoder_gain = command_line_args.encoder_gain\n",
    "encoder_param.set_image_size((im_tensor.shape[2], im_tensor.shape[3]))\n",
    "encoder_param.layers_synthesis = change_n_out_synth(\n",
    "    encoder_param.layers_synthesis, 9 if args[\"use_color_regression\"] else 6\n",
    ")\n",
    "encoder_param.use_image_arm = command_line_args.use_image_arm\n",
    "encoder_param.multi_region_image_arm = args[\"multi_region_image_arm\"]\n",
    "coolchic = CoolChicEncoder(param=encoder_param)\n",
    "coolchic.to_device(\"cuda:0\")\n",
    "# ==========================================================================================\n",
    "# SETUP LOGGER\n",
    "# ==========================================================================================\n",
    "dataset_name = im_path.split(\"/\")[-2]\n",
    "logger = TrainingLogger(\n",
    "    log_folder_path=args[\"LOG_PATH\"],\n",
    "    image_name=f\"{dataset_name}_\" + im_path.split(\"/\")[-1].split(\".\")[0],\n",
    "    debug_mode=image_encoder_manager.n_itr < 1000,\n",
    "    experiment_name=command_line_args.experiment_name,\n",
    "    write_to_disk=False,\n",
    ")\n",
    "with open(args[\"network_yaml_path\"], \"r\") as f:\n",
    "    network_yaml = f.read()\n",
    "# logger.log_result(f\"Preset: {image_encoder_manager.preset.pretty_string()}\")\n",
    "# logger.log_result(f\"Network YAML configuration:\\n{network_yaml}\")\n",
    "# logger.log_result(f\"{str_args(args)}\")\n",
    "# logger.log_result(f\"Processing image {im_path}\")\n",
    "# logger.log_result(\n",
    "#     f\"Using color space {command_line_args.color_space} with bitdepths {image_encoder_manager.colorspace_bitdepths.bitdepths}\"\n",
    "# )\n",
    "logger.log_result(f\"Using image ARM: {command_line_args.use_image_arm}\")\n",
    "logger.log_result(f\"Using encoder gain: {command_line_args.encoder_gain}\")\n",
    "logger.log_result(f\"Using multi-region image ARM: {args['multi_region_image_arm']}\")\n",
    "logger.log_result(f\"Using color regression: {args['use_color_regression']}\")\n",
    "with torch.no_grad():\n",
    "    logger.log_result(f\"Total MAC per pixel: {coolchic.get_total_mac_per_pixel()}\")\n",
    "    # logger.log_result(coolchic.str_complexity())\n",
    "# ==========================================================================================\n",
    "# TRAIN\n",
    "# ==========================================================================================\n",
    "coolchic.load_state_dict(\n",
    "    torch.load(\n",
    "        \"../logs/full_runs/28_12_2025_debug/trained_models/2025_12_30__18_49_40__trained_coolchic_kodak_kodim01_img_rate_3.2492144107818604.pth\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341097f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate per module: {'arm': {'bias': 0.0, 'weight': 0.0}, 'upsampling': {'bias': 0.0, 'weight': 0.0}, 'synthesis': {'bias': 0.0, 'weight': 0.0}},\n",
      "Final results after quantization: Loss: 3.241143226623535, Rate NN: 0.08528645833333333, Rate Latent: 0.25513243675231934, Rate Img: 2.900724411010742\n"
     ]
    }
   ],
   "source": [
    "rate_per_module, total_network_rate = coolchic.get_network_rate()\n",
    "if command_line_args.use_image_arm:\n",
    "    arm_params = list(coolchic.image_arm.parameters())\n",
    "    arm_params_bits = sum(p.numel() for p in arm_params) * 32\n",
    "    total_network_rate += arm_params_bits\n",
    "total_network_rate = float(total_network_rate) / im_tensor.numel()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Forward pass with no quantization noise\n",
    "    predicted_prior = coolchic.forward(\n",
    "        image=im_tensor,\n",
    "        quantizer_noise_type=\"none\",\n",
    "        quantizer_type=\"hardround\",\n",
    "        AC_MAX_VAL=-1,\n",
    "        flag_additional_outputs=False,\n",
    "    )\n",
    "    predicted_priors_rates = loss_function(\n",
    "        predicted_prior,\n",
    "        im_tensor,\n",
    "        rate_mlp_bpd=total_network_rate,\n",
    "        colorspace_bitdepths=colorspace_bitdepths,\n",
    "    )\n",
    "\n",
    "logger.log_result(\n",
    "    f\"Rate per module: {rate_per_module},\\n\"\n",
    "    f\"Final results before quantization: {predicted_priors_rates}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68b4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from lossless.component.types import DescriptorNN\n",
    "from lossless.nnquant.quantstep import POSSIBLE_Q_STEP, get_q_step_from_parameter_name\n",
    "from lossless.training.loss import LossFunctionOutput, loss_function\n",
    "from lossless.training.manager import ImageEncoderManager\n",
    "import itertools\n",
    "from typing import Optional, OrderedDict\n",
    "\n",
    "POSSIBLE_EXP_GOL_COUNT = {\n",
    "    \"arm\": {\n",
    "        \"weight\": torch.linspace(0, 12, 13, device=\"cpu\"),\n",
    "        \"bias\": torch.linspace(0, 12, 13, device=\"cpu\"),\n",
    "    },\n",
    "    \"image_arm\": {\n",
    "        \"weight\": torch.linspace(0, 12, 13, device=\"cpu\"),\n",
    "        \"bias\": torch.linspace(0, 12, 13, device=\"cpu\"),\n",
    "    },\n",
    "    \"upsampling\": {\n",
    "        \"weight\": torch.linspace(0, 12, 13, device=\"cpu\"),\n",
    "        \"bias\": torch.linspace(0, 12, 13, device=\"cpu\"),\n",
    "    },\n",
    "    \"synthesis\": {\n",
    "        \"weight\": torch.linspace(0, 12, 13, device=\"cpu\"),\n",
    "        \"bias\": torch.linspace(0, 12, 13, device=\"cpu\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "def exp_golomb_nbins(symbol: torch.Tensor, count: int = 0) -> torch.Tensor:\n",
    "    \"\"\"Compute the number of bits required to encode a Tensor of integers\n",
    "    using an exponential-golomb code with exponent ``count``.\n",
    "\n",
    "    Args:\n",
    "        symbol: Tensor to encode\n",
    "        count (int, optional): Exponent of the exp-golomb code. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Number of bits required to encode all the symbols.\n",
    "    \"\"\"\n",
    "\n",
    "    # We encode the sign equiprobably at the end thus one more bit if symbol != 0\n",
    "    nbins = (\n",
    "        2 * torch.floor(torch.log2(symbol.abs() / (2**count) + 1))\n",
    "        + count\n",
    "        + 1\n",
    "        + (symbol != 0)\n",
    "    )\n",
    "    res = nbins.sum()\n",
    "    return res\n",
    "\n",
    "\n",
    "POSSIBLE_Q_STEP_SHIFT = {\n",
    "    \"arm\": {\n",
    "        \"weight\": torch.linspace(-8, 0, 9, device=\"cpu\"),\n",
    "        \"bias\": torch.linspace(-16, 0, 17, device=\"cpu\"),\n",
    "    },\n",
    "    \"image_arm\": {\n",
    "        \"weight\": torch.linspace(-8, 0, 9, device=\"cpu\"),\n",
    "        \"bias\": torch.linspace(-16, 0, 17, device=\"cpu\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "POSSIBLE_Q_STEP = {\n",
    "    \"arm\": {\n",
    "        \"weight\": 2.0 ** POSSIBLE_Q_STEP_SHIFT[\"arm\"][\"weight\"],\n",
    "        \"bias\": 2.0 ** POSSIBLE_Q_STEP_SHIFT[\"arm\"][\"bias\"],\n",
    "    },\n",
    "    \"image_arm\": {\n",
    "        \"weight\": 2.0 ** torch.linspace(-12, 0, 13, device=\"cpu\"),\n",
    "        \"bias\": 2.0 ** torch.linspace(-24, 0, 25, device=\"cpu\"),\n",
    "    },\n",
    "    \"upsampling\": {\n",
    "        \"weight\": 2.0 ** torch.linspace(-12, 0, 13, device=\"cpu\"),\n",
    "        \"bias\": 2.0 ** torch.tensor([0.0]),\n",
    "    },\n",
    "    \"synthesis\": {\n",
    "        \"weight\": 2.0 ** torch.linspace(-12, 0, 13, device=\"cpu\"),\n",
    "        \"bias\": 2.0 ** torch.linspace(-24, 0, 25, device=\"cpu\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_q_step_from_parameter_name(parameter_name: str, q_step: DescriptorNN) -> Optional[float]:\n",
    "    \"\"\"Return the specific quantization step from q_step (a dictionary\n",
    "    with several quantization steps). The specific quantization step is\n",
    "    selected through the parameter name.\n",
    "\n",
    "    Args:\n",
    "        parameter_name (str): Name of the parameter in the state dict.\n",
    "        q_step (DescriptorNN): Dictionary gathering several quantization\n",
    "            steps. E.g. one quantization step for the weights and one for\n",
    "            the biases.\n",
    "\n",
    "    Returns:\n",
    "        Optional[float]: The quantization step associated to the parameter.\n",
    "            Return None if nothing is found.\n",
    "    \"\"\"\n",
    "    if \".weight\" in parameter_name:\n",
    "        current_q_step = q_step[\"weight\"]\n",
    "    elif \".bias\" in parameter_name:\n",
    "        current_q_step = q_step[\"bias\"]\n",
    "    else:\n",
    "        print('Parameter name should include \".weight\" or \".bias\" ' f\"Found: {parameter_name}\")\n",
    "        current_q_step = None\n",
    "\n",
    "    return current_q_step\n",
    "\n",
    "\n",
    "def _quantize_parameters(\n",
    "    fp_param: OrderedDict[str, torch.Tensor],\n",
    "    q_step: DescriptorNN,\n",
    ") -> Optional[OrderedDict[str, torch.Tensor]]:\n",
    "    MAX_AC_MAX_VAL = 65535  # 2**16 for 16-bit code in bitstream header.\n",
    "\n",
    "    q_param = OrderedDict()\n",
    "    for k, v in fp_param.items():\n",
    "        current_q_step = get_q_step_from_parameter_name(k, q_step)\n",
    "        # supress type warning\n",
    "        assert current_q_step is not None\n",
    "        sent_param = torch.round(v / current_q_step)\n",
    "\n",
    "        if sent_param.abs().max() > MAX_AC_MAX_VAL:\n",
    "            return None\n",
    "\n",
    "        q_param[k] = sent_param * current_q_step\n",
    "\n",
    "    return q_param\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_model(\n",
    "    model: CoolChicEncoder,\n",
    "    image: torch.Tensor,\n",
    "    image_encoder_manager: ImageEncoderManager,\n",
    "    logger: TrainingLogger,\n",
    ") -> CoolChicEncoder:\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    # We have to quantize all the modules that we want to send\n",
    "    module_to_quantize = {\n",
    "        module_name: getattr(model, module_name) for module_name in model.modules_to_send\n",
    "    }\n",
    "\n",
    "    for module_name, cur_module in sorted(module_to_quantize.items()):\n",
    "        # Start the RD optimization for the quantization step of each module with an\n",
    "        # arbitrary high value for the RD cost.\n",
    "        best_loss = LossFunctionOutput(\n",
    "            loss=torch.Tensor([1e6]).to(image.device),\n",
    "            rate_nn_bpd=0.0,\n",
    "            rate_latent_bpd=0.0,\n",
    "            rate_img_bpd=1e6,\n",
    "        )\n",
    "\n",
    "        # All possible quantization steps for this module\n",
    "        all_q_step = POSSIBLE_Q_STEP.get(module_name)\n",
    "        all_expgol_cnt = POSSIBLE_EXP_GOL_COUNT.get(module_name)\n",
    "        assert all_q_step is not None\n",
    "        assert all_expgol_cnt is not None\n",
    "\n",
    "        # Save full precision parameter.\n",
    "        full_precision_param = cur_module.get_param()\n",
    "\n",
    "        best_q_step: DescriptorNN = DescriptorNN()\n",
    "        # Overall best expgol count for this module weights and biases\n",
    "        final_best_expgol_cnt = DescriptorNN()\n",
    "        all_q_step_weight = all_q_step.get(\"weight\")\n",
    "        all_q_step_bias = all_q_step.get(\"bias\")\n",
    "        assert all_q_step_weight is not None\n",
    "        assert all_q_step_bias is not None\n",
    "\n",
    "        for q_step_w, q_step_b in itertools.product(all_q_step_weight, all_q_step_bias):\n",
    "            # Reset full precision parameters, set the quantization step\n",
    "            # and quantize the model.\n",
    "            current_q_step: DescriptorNN = DescriptorNN(\n",
    "                weight=q_step_w,\n",
    "                bias=q_step_b,\n",
    "            )\n",
    "\n",
    "            # Reset full precision parameter before quantizing\n",
    "            q_param = _quantize_parameters(full_precision_param, current_q_step)\n",
    "\n",
    "            # Quantization has failed\n",
    "            if q_param is None:\n",
    "                continue\n",
    "\n",
    "            cur_module.set_param(q_param)\n",
    "\n",
    "            # Plug the quantized module back into Cool-chic\n",
    "            # setattr(frame_encoder.coolchic_enc[cc_name], module_name, cur_module)\n",
    "            setattr(model, module_name, cur_module)\n",
    "\n",
    "            model.nn_q_step[module_name] = current_q_step\n",
    "\n",
    "            # Test Cool-chic performance with this quantization steps pair\n",
    "            frame_encoder_out = model.forward(\n",
    "                image=image,\n",
    "                quantizer_noise_type=\"none\",\n",
    "                quantizer_type=\"hardround\",\n",
    "                AC_MAX_VAL=-1,\n",
    "                flag_additional_outputs=False,\n",
    "            )\n",
    "\n",
    "            param = cur_module.get_param()\n",
    "\n",
    "            # Best exp-golomb count for this quantization step\n",
    "            best_expgol_cnt = DescriptorNN()\n",
    "            for weight_or_bias in [\"weight\", \"bias\"]:\n",
    "\n",
    "                # Find the best exp-golomb count for this quantization step:\n",
    "                cur_best_expgol_cnt = None\n",
    "                # Arbitrarily high number\n",
    "                cur_best_rate = 1e9\n",
    "\n",
    "                sent_param = []\n",
    "                for parameter_name, parameter_value in param.items():\n",
    "\n",
    "                    # Quantization is round(parameter_value / q_step) * q_step so we divide by q_step\n",
    "                    # to obtain the sent latent.\n",
    "                    current_sent_param = (parameter_value / current_q_step[weight_or_bias]).view(-1)\n",
    "\n",
    "                    if weight_or_bias in parameter_name:\n",
    "                        sent_param.append(current_sent_param)\n",
    "\n",
    "                # Integer, sent parameters\n",
    "                v = torch.cat(sent_param)\n",
    "\n",
    "                # Find the best expgol count for this weight\n",
    "                for expgol_cnt in all_expgol_cnt[weight_or_bias]:\n",
    "                    cur_rate = exp_golomb_nbins(v, count=int(expgol_cnt))\n",
    "                    if cur_rate < cur_best_rate:\n",
    "                        cur_best_rate = cur_rate\n",
    "                        cur_best_expgol_cnt = expgol_cnt\n",
    "\n",
    "                assert cur_best_expgol_cnt is not None\n",
    "                best_expgol_cnt[weight_or_bias] = cur_best_expgol_cnt\n",
    "\n",
    "            model.nn_expgol_cnt[module_name] = best_expgol_cnt\n",
    "\n",
    "            _, total_rate_nn_bit = model.get_network_rate()\n",
    "\n",
    "            loss_fn_output = loss_function(\n",
    "                frame_encoder_out,\n",
    "                image,\n",
    "                rate_mlp_bpd=total_rate_nn_bit / image.numel(),\n",
    "                latent_multiplier=1.0,\n",
    "                colorspace_bitdepths=image_encoder_manager.colorspace_bitdepths,\n",
    "            )\n",
    "\n",
    "            # Store best quantization steps\n",
    "            if loss_fn_output.loss < best_loss.loss:\n",
    "                best_loss = loss_fn_output\n",
    "                best_q_step = current_q_step\n",
    "                final_best_expgol_cnt = best_expgol_cnt\n",
    "                if module_name == \"image_arm\":\n",
    "                    print(\n",
    "                        loss_fn_output.loss.cpu().item(),\n",
    "                        q_step_w.cpu().item(),\n",
    "                        q_step_b.cpu().item(),\n",
    "                    )\n",
    "\n",
    "        # Once we've tested all the possible quantization step and expgol_cnt,\n",
    "        # quantize one last time with the best one we've found to actually use it.\n",
    "        model.nn_q_step[module_name] = best_q_step\n",
    "        model.nn_expgol_cnt[module_name] = final_best_expgol_cnt\n",
    "\n",
    "        q_param = _quantize_parameters(full_precision_param, model.nn_q_step[module_name])\n",
    "        assert q_param is not None, (\n",
    "            \"_quantize_parameters() failed with q_step \" f\"{model.nn_q_step[module_name]}\"\n",
    "        )\n",
    "        if logger is not None:\n",
    "            logger.log_result(f\"Best loss for module {module_name}: {best_loss}\")\n",
    "        else:\n",
    "            print(f\"Best loss for module {module_name}: {best_loss}\")\n",
    "\n",
    "        cur_module.set_param(q_param)\n",
    "        # Plug the quantized module back into Cool-chic\n",
    "        setattr(model, module_name, cur_module)\n",
    "\n",
    "    time_nn_quantization = time.time() - start_time\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.log_result(f\"\\nTime quantize_model(): {time_nn_quantization:4.1f} seconds\\n\")\n",
    "    else:\n",
    "        print(f\"\\nTime quantize_model(): {time_nn_quantization:4.1f} seconds\\n\")\n",
    "    if image_encoder_manager is not None:\n",
    "        image_encoder_manager.total_training_time_sec += time_nn_quantization\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
